{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten notebook jest oceniany półautomatycznie. Nie twórz ani nie usuwaj komórek - struktura notebooka musi zostać zachowana. Odpowiedź wypełnij tam gdzie jest na to wskazane miejsce - odpowiedzi w innych miejscach nie będą sprawdzane (nie są widoczne dla sprawdzającego w systemie).\n",
    "\n",
    "W szczególności zwróć uwagę, że usupełniłeś wszystkie miejsca `YOUR CODE HERE`, `WPISZ TWÓJ KOD TUTAJ`, \"YOUR ANSWER HERE\" lub \"WPISZ TWOJĄ ODPOWIEDŹ TUTAJ\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zaawansowane Przetwarzanie Języka Naturalnego\n",
    "# Laboratorium 2\n",
    "\n",
    "Pobierz zbiór danych Amazon \"Musical Instruments\" z [tej](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz) strony internetowej, a następnie wczytaj go poniższym kodem. Zwróć uwagę na wymaganą lokalizację pliku, tj. dwa katalogi wyżej - wynika to ze struktury plików w sprawdzarce, przepraszam za niedogodność.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    " \n",
    "x_text = []\n",
    "y = []\n",
    "with open('../../Musical_Instruments_5.json') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        x_text.append(data['reviewText'].lower().strip())\n",
    "        y.append(int(data['overall']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 - przygotowanie danych\n",
    "W załadowanych listach `x_text` oraz `y` znajdują się odpowiednio teksty kolejnych opinii oraz oznaczenia klas. Klasą w tym wypadku jest liczba gwiazdek (ocena) produktu towarzysząca opinii. Zadanie klasyfikacji polega na przewidzeniu oceny na podstawie opinii pozostawionej w portalu.\n",
    "\n",
    "Aby zmniejszyć wymagania obliczeniowe do dalszych eksperymentów, ograniczymy zbiór danych jedynie do pierwszego tysiąca opinii.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text = x_text[:1000]\n",
    "y = y[:1000]\n",
    "train_end_idx=int(0.9 * len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedną z użytecznych operacji przygotowania tekstu do konstrukcji klasyfikatora jest zastąpienie poszczególnych tokenów ich indeksami. Chociaż w praktyce ten proces często następuje dopiero po szeregu etapów przetwarzania tekstu takich jak tokenizacja, lematyzacja czy stemming - w tym ćwiczeniu wyodrębnimy tokeny rozdzielając tekst znakiem spacji.\n",
    "\n",
    "Klasyfikator powinien obsługiwać także słowa, które nie występowały w zbiorze uczącym. Podstawową techniką obsługi takich słów jest wprowadzenie specjalnego tokenu UNK, obsługującego nieznane słowa. W tym celu usuwa się ze zbioru danych pewną liczbę najrzadszych słów i zastępuje się je tokenami UNK.\n",
    "\n",
    "Zbuduj słownik `w2i` mapujący tokeny na kolejne indeksy tj. liczby naturalne. Pomiń tokeny występujące w zbiorze uczącym 5 lub mniej razy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f866dd9918a27f9ba2f31ef62a061fa1",
     "grade": false,
     "grade_id": "cell-87edb8b6e5279a0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))\n",
    "UNK = w2i[\"<unk>\"] #Przypisz indeks tokenowi UNK\n",
    "\n",
    "# WPISZ TWÓJ KOD TUTAJ\n",
    "raise NotImplementedError()\n",
    "n_words = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b1000affcd1be506cad1e3a22a63cf1",
     "grade": true,
     "grade_id": "cell-59701c5db1041735",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po zbudowaniu słownika `w2i`, przekonwertujmy nasz zbiór danych z listy słów na listę indeksów słów. Od razu podzielimy zbiór na część uczącą i część testową, a także przekonwertujemy klasy na indeksy klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: UNK, w2i) # Domyślną wartością słownika jest UNK, \n",
    "         #chociaż w2i będzie zawierał wpisy do wszystkich słów to nowym tokenom będzie przypisywał indeks UNK\n",
    "class2i = defaultdict(lambda: len(class2i))\n",
    "        # mapuj klasy na indeksy klas\n",
    "    \n",
    "def read_dataset(start_idx,end_idx):\n",
    "    for i, text in enumerate(x_text[start_idx:end_idx]):\n",
    "        yield ([w2i[x] for x in text.split(\" \")], class2i[y[i]])\n",
    "        \n",
    "train = list(read_dataset(0, train_end_idx))\n",
    "dev = list(read_dataset(train_end_idx, len(y)))\n",
    "n_class = len(class2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_words, n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2 - pierwszy model klasyfikacji tekstu w PyTorch\n",
    "Podstawową strukturą danych w PyTorch jest tensor, na którym możesz wykonywać analogiczne operacje jak na macierzach `numpy`. Podstawową metodą stworzenia tensora jest wywołanie konstruktora `torch.tensor` na liście liczb. Istnieją także inne konstruktory tensorów, analogiczne do `numpy`. Można też na nich operować za pomocą standardowych operatorów, indeksowania, i odpowiedników innych funkcji znanych z `numpy`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor([1,2,3]))\n",
    "print(torch.rand( (3,3) ))\n",
    "print(torch.ones( (3,3) ))\n",
    "print(2 * torch.ones( (3,3) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dlaczego więc korzystamy z PyTorch, a nie z biblioteki `numpy` skoro tensory wydają się mieć analogiczną funkcjonalność do poznanych uprzednio macierzy? Powodów jest oczywiście wiele, m.in. możliwość przeniesienia obliczeń na kartę graficzną (technologia CUDA), ale z punktu widzenia naszego ćwiczenia kluczowa jest funkcjonalność automatycznego liczenia gradientów. W przypadku konstrukcji sieci neuronowej czy modelu liniowego, w PyTorch nie jest konieczne samodzielne wyprowadzanie i implementowanie gradientu, gdyż biblioteka zrobi to za nas automatycznie.\n",
    "\n",
    "Wyznaczanie gradientów odbywa się za pomocą algorytmu wstecznej propagacji, który ma dwie fazy: *forward* i *backward*. Faza *forward* polega na policzeniu wyniku funkcji, a faza *backward* wyznacza gradienty wszystkich jej parametrów.\n",
    "\n",
    "W celu poznania tej funkcjonalności policzymy pochodne cząstkowe prostej funkcji kwadratowej:\n",
    "$$result = x_1^2 + x_2^2+ x_3^2$$\n",
    "której pochodne cząstkowe mają postać $2x_i$.\n",
    "\n",
    "Rozpocznijmy implementacje tej funkcji od stworzenia 3-elementowego wektora zmiennych `x`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.,2.,3.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak pewnie zauważyłeś, w konstruktorze użyliśmy dodatkowego parametru `requires_grad`. Domyślnie wykonanie operacji na dowolnym tensorze nie traktuje się jako części fazy `forward`, gdyż nie do wszystkich tensorów użytych w kodzie będziemy potrzebować wartości pochodnych. Aby zasygnalizować, że dla danej zmiennej konieczne jest zapisywanie informacji o wykonywanych na niej operacjach, należy ustawić wartość jej parametru `requires_grad` na `True`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przejdźmy do policzenia wartości wyżej zdefiniowanej funkcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (x**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensory posiadają parametr `.grad`, który przechowuje informacje o wyznaczonym gradiencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tej chwili, pomimo obliczenia wartości zmiennej `result`, wartość gradientu nie jest policzona, gdyż nie poinformowaliśmy biblioteki o zakończeniu fazy `forward` i konieczności wykonania fazy `backward`. Możemy to zrobić poprzez wykonanie funkcji `backward()` na obliczonej wartości funkcji (funkcję tę można wywołać tylko na skalarnym wyniku!).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że wektor `x.grad`, zgodnie z naszymi oczekiwaniami, zawiera wartości pochodnej cząstkowej tj. `2x`. Spróbujmy jeszcze raz, licząc pochodną po logarytmie z `result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = torch.log(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niestety operacja się nie powiodła. Przed wykonaniem kolejnej fazy *backward* należy - upraszczając - wykonać fazę *forward*. Nasze poprzednie operacje konstruowały fazę *forward* od parametrów `x` aż do zmiennej z wynikiem, jednakże przy wykonaniu fazy *backward* została zwolniona pamięć przechowująca informacje o kolejno wykonywanych operacjach na tych zmiennych (graf obliczeń). Kolejna operacja została wykonana bezpośrednio na tensorze `result`, konstruując fazę *forward* od `result` do `result2`, jednak zabrakło grafu obliczeń od parametrów `x`.\n",
    "\n",
    "Uruchomienie poniższego kodu, z operacjami rozpoczynającymi się od `x`, zakończy się obliczeniem gradientu z sukcesem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (x**2).sum()\n",
    "result2 = torch.log(result)\n",
    "result2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy poprawność uzyskanego wyniku. Zmienna $result= 1^2+2^2+3^2=14$, a pochodna z logarytmu naturalnego to $\\frac{1}{x}$. W związku z tym:\n",
    "$$\\frac{\\partial }{\\partial x_1} \\log result = \\frac{1}{result} \\cdot \\frac{\\partial }{\\partial x_1} result = \\frac{1}{result} 2x_1 $$\n",
    "Przy naszych wartościach $x$ równa się to $\\frac{1}{14}\\cdot 2 = 0,1428$. Łatwo zauważyć, że wynik znajdujący się w tensorze `x.grad` jest błędny, a konkretnie za duży o 2 jednostki.\n",
    "\n",
    "Stało się tak dlatego, że gradient z kolejnych faz `backward` jest akumulowany w parametrze `.grad` (poprzednia wartość policzonej pochodnej cząstkowej wynosiła właśnie 2). Takie zachowanie biblioteki może być bardzo użyteczne w sytuacji gdy chcemy w zmiennej zagregować gradienty funkcji celu liczonych na kolejno przetwarzanych instancjach lub przy treningu modelu z wieloma funkcjami celu; tutaj jednak doprowadziło to do błędnego wyniku. Z tego powodu bardzo ważne jest pamiętanie o wyzerowaniu wartości gradientów przed przystąpieniem do kolejnych obliczeń.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (x**2).sum()\n",
    "result2 = torch.log(result)\n",
    "result2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę na konwencję biblioteki PyTorch - jeśli nazwa funkcji zakończona jest podkreślnikiem to taka operacja jest wykonywana `in-place`. (np. `x.add(5)` - `x` nadal ma stałą wartość, `x.add_(5)` wartość `x` zwiększono o 5).\n",
    "\n",
    "Zaimplementujmy podstawowy algorytm uczący w PyTorch. Będzie to prosta sieć neuronowa składająca się z:\n",
    "- macierzy zanurzeń $C$, przetwarzającej indeksy słów na odpowiednie reprezentacje wektorowe, \n",
    "- operacji uśredniania tych zanurzeń do jednego zanurzenia (average pooling over time) \n",
    "- oraz jednej warstwy liniowej (softmax) zwracającej wynik.\n",
    "\n",
    "Algorytmem uczącym będzie SGD optymalizujące entropię krzyżową, czyli dla kolejnych instancji uczących będziemy wykonywać:\n",
    "$$parametry = parametry - \\eta \\nabla f\\_celu$$\n",
    "Implementacja ta będzie wyjątkowo prosta, gdyż gradient funkcji celu ($\\nabla f\\_celu$) zostanie obliczony automatycznie przez PyTorch. Ponadto entropia krzyżowa jest już zaimplementowana w PyTorch `F.cross_entropy(logits, target)`. Zwróć uwagę, że argumentem tej funkcji są wartości logitów (nie trzeba implementować funkcji softmax przetwarzającej wartości logitów na prawdopodobieństwa).\n",
    "\n",
    "**UWAGA** W implementacji nie należy używać gotowych implementacji SGD czy warstw sieci neuronowych w PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszym krokiem w implementacji będzie zaimplementowanie samego modelu. Należy zainicjalizować macierz $C$ przechowującą w wierszach zanurzenia dla kolejnych słów (liczba słów to `n_words`, wymiarowość zanurzenia określ na 20) oraz macierz $W$ przechowującą parametry warstwy liniowej, zwracającej wartości logitów dla każdej z klas (liczba klas to `n_class`). Macierz $W$ w dodatkowej kolumnie powinna też przechowywać wartości wyrazów wolnych (bias). Wartości zainicjalizuj losowo `torch.rand`. Pamiętaj, że dla tych macierzy będziesz potrzebował wyznaczyć potem wartości gradientów.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "134cf7cd8660133b178caff1ca2fcdfc",
     "grade": false,
     "grade_id": "cell-9ba79ce47d0c6a5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 20\n",
    "#C = torch.rand(....)\n",
    "#W = torch.rand(....)\n",
    "\n",
    "# WPISZ TWÓJ KOD TUTAJ\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "084514b8fbf6ffa4f784b5b398bd3624",
     "grade": true,
     "grade_id": "cell-b3436825d33d5941",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj funkcję `simple_model`, której argumentem będzie instancja testowa (jest to więc lista indeksów słów występujących w tekście), a na której wyjściu będzie wektor `n_class`-elementowy zawierający obliczone wartości logitów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "280f8962c6b7f20c718a51b6ecc294a4",
     "grade": false,
     "grade_id": "cell-e2668a9abacbc78a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_model(x):\n",
    "    # doc_embedding = średnia z wektorów kolejnych słów\n",
    "    # WPISZ TWÓJ KOD TUTAJ\n",
    "    raise NotImplementedError()\n",
    "    doc_with_bias = torch.cat([torch.ones(1),doc_embedding]) # Skonkatenowanie 1 z uzyskaną reprezentacją (bias)\n",
    "    return W @ doc_with_bias # Obliczenie wartości logitów (tj. warstwa liniowa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj algorytm SGD w poniższej pętli. Pętla ta iteruje po zbiorze uczącym oraz dla każdej instancji oblicza wartość funkcji celu. Twoje zadania:\n",
    "- Policz gradienty (faza *backward*)\n",
    "- Zaimplementuj aktualizacje $W$ i $C$ wg. wzoru na SGD. Operacje modyfikujące $W$ i $C$ musisz wykonać w środku klauzuli `with torch.no_grad():`, aby nie śledzić z tych operacji gradientów.\n",
    "- Pamiętaj o wyczyszczeniu gradientów (zarówno w $W$ jak i $C$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80acc452845b941b086f5aa582f26d87",
     "grade": false,
     "grade_id": "cell-aeedd0f5183bd781",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        pred = simple_model(words)\n",
    "        loss = F.cross_entropy(pred.reshape(1,-1), torch.tensor(tag).reshape(1))\n",
    "        # Loss zawiera wartość funkcji celu dla przykładu, wykonaj backpropagation\n",
    "        # WPISZ TWÓJ KOD TUTAJ\n",
    "        raise NotImplementedError()\n",
    "        train_loss += loss\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Dlaczego w bibliotekach do głębokiego uczenia maszynowego, takich jak PyTorch, implementuje się funkcje entropii krzyżowej tak, aby przyjmowała na wejście wartości logitów zamiast prawdopodobieństw z softmax?\n",
    "- Na wykładzie pokazywaliśmy warstwę zanurzeń jako warstwę mnożącą macierz $C$ przez wektor \"1 z n\", można ją jednak także zaimplementować jako operację odczytu odpowiedniego wiersza z macierzy. Jakie są wady i zalety obu tych sposobów implementacji?\n",
    "\n",
    "Odpowiedź na pierwszą kropkę umieść poniżej.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5158398dc380a8a01088a617375d8f4c",
     "grade": true,
     "grade_id": "cell-c685493684bda3e3",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "WPISZ TWOJĄ ODPOWIEDŹ TUTAJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3 - wykorzystanie nn.Module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch jako biblioteka do głębokiego uczenia maszynowego oferuje nam kilka udogodnień w implementowaniu modeli uczących się, aby jeszcze bardziej uprościć ich implementację. Większość z tych udogodnień związanych z jest z reprezentowaniem modeli uczących się jako obiektów dziedziczących po `torch.nn.Module`. W obiekcie takim powinniśmy zaimplementować co najmniej konstruktor, inicjalizujący parametry modelu ($W$ i $C$), oraz funkcję `forward` obliczającą wynik modelu (we wcześniejszym zadaniu nazywaliśmy ją `simple_model`). Ponadto moduł `torch.nn` oferuje gotowe implementacje zarówno warstwy liniowej jak i warstwy zanurzeń.\n",
    "\n",
    "Przeanalizuj poniższą implementację modelu z poprzedniego zadania.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, n_words, emb_size, n_class):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(n_words, emb_size)\n",
    "        self.linear = torch.nn.Linear(in_features=emb_size, out_features=n_class, bias=True)\n",
    "        torch.nn.init.uniform_(self.embedding.weight, -0.25, 0.25)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words)                 \n",
    "        h = emb.mean(dim=0)                         \n",
    "        h = torch.reshape(h, (1,-1))\n",
    "        out = self.linear(h)              \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oprócz tego, że uzyskaliśmy elegancki obiekt reprezentujący nasz model, nie wydaje się by powyższa implementacja była krótsza czy prostsza od tej, którą uzyskaliśmy w poprzednim zadaniu bez dobrodziejstw `nn.Module`. Co zatem zyskaliśmy?\n",
    "\n",
    "Przy implementacji modeli z dużą liczbą warstw, szczególnie uciążliwe byłoby implementowanie kolejnych linijek kodu zerujących gradienty wszystkich macierzy wag, oraz wykonywanie na nich kroków algorytmu SGD. W naszej implementacji każda macierz parametrów to dwie linijki kodu! Jednak modele dziedziczące po `torch.nn.Module` i stworzone poprzez dedykowane warstwy neuronowe posiadają gotową funkcję `parameters()` zwracającą kolejne macierze parametrów modelu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel(n_words, EMBEDDING_SIZE, n_class)\n",
    "print([i for i in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jest to niezwykle wygodne, bo implementacja algorytmu SGD może przeiterować po tej liście parametrów i dla każdej z nich wykonać aktualizację ich wartości. Fakt, że taka lista jest tworzona automatycznie pozbawia nas ryzyka, że zwyczajnie o którejś macierzy parametrów czy wektorze wyrazów wolnych najzwyczajniej zapomnimy. \n",
    "\n",
    "Podobnie można zaimplementować pętlę zerującą gradienty wszystkich parametrów. Modele oferują nawet gotową taką funkcję `model.zero_grad()`, która iteruje po parametrach zerując ich gradienty. \n",
    "\n",
    "Zmodyfikuj implementację SGD z poprzedniego zadania, tak aby wykorzystywała `zero_grad()` i `parameters()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5f2f3a6cb7d7b2d2e34e63a2b54004b",
     "grade": false,
     "grade_id": "cell-87d2dfc0c801725a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        # WPISZ TWÓJ KOD TUTAJ\n",
    "        raise NotImplementedError()\n",
    "        train_loss += loss\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo moduł `torch.nn` oferuje także od razu zaimplementowane optymalizatory, w tym SGD. W konstruktorze optymalizatora należy podać listę optymalizowanych przez niego parametrów, a następnie wywołać na nim procedurę `step()` wykonującą krok algorytmu optymalizacyjnego tj. aktualizację wartości zmiennych przy użyciu gradientu. W tej sytuacji nie musisz się martwić o umieszczanie kodu zmieniającego parametry w `with torch.no_grad()` - optymalizator sam to zrobi! Optymalizator również oferuje funkcję `zero_grad()`, zerującą gradienty zmiennych wskazanych do optymalizacji.\n",
    "\n",
    "Zmodyfikuj kod z poprzedniego zadania, tak aby wykorzystywał optymalizator SGD zaimplementowany w `torch.optim`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92b92a0626a6c533b3b61aa2e6c18ddd",
     "grade": false,
     "grade_id": "cell-5588a40beb11d256",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "\n",
    "#optimizer = ...\n",
    "# WPISZ TWÓJ KOD TUTAJ\n",
    "raise NotImplementedError()\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        # WPISZ TWÓJ KOD TUTAJ\n",
    "        raise NotImplementedError()\n",
    "        train_loss += loss\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Przeanalizuj dokładnie powyższy kod i przechodząc linia po linii, wyjaśnij co one robią z punktu widzenia treningu modelu.\n",
    "- Zastanów się jak wyglądałaby Twoja własna implementacja klasy `optim.SGD`.\n",
    "- Prześledź jeszcze raz implementację modelu neuronowego - pewnie w niedługim czasie będziesz implementował znacznie bardziej skomplikowane modele, tym bardziej warto je dobrze prześledzić!\n",
    "- Czym różni się zaimplementowana architektura od głębokiej sieci uśredniającej?\n",
    "- Na wykładzie korzystaliśmy z macierzy zanurzeń w modelach języka. Tutaj warstwa zanurzeń pojawiła się bezpośrednio w modelu klasyfikacji. Czy w uzyskanych w ten sposób zanurzeniach (zakładając dobry dobór hiperparamerów, dodanie regularyzacji itd.) zaobserwowalibyśmy podobne zależności jak te uzyskane za pomocą modelu języka? Jeśli nie, obserwacji jakich zależności między słowami spodziewałbyś się w tej reprezentacji? Skąd biorą się różnice?\n",
    "\n",
    "Odpowiedź na ostatnią kropkę umieść poniżej.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "731e0439a761d52c3124060a972eaf31",
     "grade": true,
     "grade_id": "cell-50c2ecade40bcd7f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "WPISZ TWOJĄ ODPOWIEDŹ TUTAJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73080ad0900acee85ddd2dd6ff52a07d",
     "grade": false,
     "grade_id": "cell-a81ef968001b7310",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Zadanie 4\n",
    "Wykorzystując wiedzę z poprzedniego zadania zaimplementuj prostą architekturę splotową do klasyfikacji tekstu i wytrenuj ją. Do jej wykonania może być przydatna klasa `torch.nn.Conv1d` i funkcja `torch.nn.ReLU` (zapoznaj się z ich dokumentacją w Internecie). Jako funkcji redukcji użyj funkcji maksimum (over time).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8e23fa15d0164d9a5260ab5ea518263",
     "grade": false,
     "grade_id": "cell-dbb714252ae2ebc7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, n_words, emb_size, num_filters, window_size, ntags):\n",
    "        super(CNNclass, self).__init__()\n",
    "        # WPISZ TWÓJ KOD TUTAJ\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, words):\n",
    "        # WPISZ TWÓJ KOD TUTAJ\n",
    "        raise NotImplementedError()\n",
    "       \n",
    "# Kod do treningu sieci\n",
    "# WPISZ TWÓJ KOD TUTAJ\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
