{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zaawansowane Przetwarzanie Języka Naturalnego\n",
    "## Laboratorium 3: Neuronowe modele predykcji sekwencji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem zadania jest zaimplementowanie neuronowego modelu do predykcji sekwencji i zastosowanie go do zadania płytkiej analizy frazowej. Zaimplementowany model będzie oparty o dwukierunkowe sieci rekurencyjne oraz zakończony będzie warstwą CRF. Celem ćwiczenia nie jest stworzenie wydajnej implementacji odpowiedniej do pracy z dużymi zbiorami treningowymi, ale pogłębienie zrozumienia działania metod modelowania predykcji poprzez ich implementację."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "Pracując w środowisku PyTorch nie jest konieczne samodzielne implementowanie neuronów rekurencyjnych, gdyż biblioteka `torch.nn` zawiera ich gotowe implementacje. Klasyczny neuron rekurencyjny z funkcją aktywacji tangens hiperboliczny jest zaimplementowany w klasie `RNN` i od razu pozwala na przetworzenie od razu całej sekwencji elementów. Konstruktor obiektu wymaga wyspecyfikowania liczby cech którymi opisany jest każdy element sekwencji, liczby neuronów w warstwie rekurenycjnej oraz liczbę warstw rekurencyjnych. \n",
    "\n",
    "Istotnym parametrem modelu jest opcjonalny przełącznik `batch_first = True` który określa format wejściowej sekwencji. Domyślnie modele rekurencyjne spodziewają się wejścia o wymiarach `(długość sekwencji, rozmiar batcha, liczba cech)` czyli w kolumnach tensora mamy kolejne sekwencje (przykłady uczące), w wierszach kolejne elementy tych sekwencji, a w głębokości tensora umieszone są kolejne cechy każdego z elementów sekwencji. Zwróć uwagę, że jest to format inny od często stosowanego formatu danych niesekwencyjnych gdzie w wierszach umieszcza się kolejne przykłady uczące, a w kolumnach kolejne cechy (tutaj jest na odwrót: przykład uczący-sekwencja umieszcona jest w kolumnie, a nie w wierszu). Jeśli jednak ustawisz przełącznik `batch_first = True` to sieć rekurencyjna będzie się spodziewała wejścia jako `(rozmiar batcha, długość sekwencji, liczba cech)` czyli kolejne sekwencje będą w kolejnych wierszach tensora.\n",
    "\n",
    "Dalej, warto zauważyć, że w przypadku np. warstwy liniowej do obliczenia wyniku wystarczyło podanie samych danych ( `linear(dane)`), jednak wejściem sieci rekurencyjnej są nie tylko dane ale i poprzednie stany ukryte $h_{t-1}$ ( `rnn(dane, h0)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:52:59.506194700Z",
     "start_time": "2024-01-10T18:52:57.346106400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyjście warstwy: tensor([[[ 0.9886, -0.6947, -0.6778],\n",
      "         [ 0.6070,  0.0042,  0.8744],\n",
      "         [ 0.9132, -0.7435,  0.9708],\n",
      "         [ 0.9568, -0.8087,  0.9874]]], grad_fn=<TransposeBackward1>)\n",
      "Ostatni stan ukryty: tensor([[[ 0.9568, -0.8087,  0.9874]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "FeatureCount = 2\n",
    "LayerCount = 1\n",
    "HiddenSize = 3\n",
    "\n",
    "# Proste przykładowe dane: jedna 4 elementowa sekwencja gdzie każdy jej element jest opisany dwoma cechami\n",
    "# Zwróć uwagę na wymiary tensora (rozmiar batcha, długość sekwencji, liczba cech elementów) \n",
    "#     - konieczne batch_first=True\n",
    "data = torch.FloatTensor([[1, 1], [2, 2], [3, 3], [4, 4]]).view(1, 4, 2)\n",
    "\n",
    "rnn = nn.RNN(FeatureCount, HiddenSize, LayerCount, batch_first=True)\n",
    "\n",
    "# Początkowy stan ukryty h0. Każda warstwa sieci rekurencyjnej zaczyna przetwarzanie od swojego h0\n",
    "# W sytuacji gdy przetwarzamy kilka sekwencji na raz (sieć jest uruchamiana równolegle na kilku sekwencjach)\n",
    "# również potrzebujemy dla każdej przetwarzanej sekwencji jej stan początkowy h0\n",
    "# Porównaj powyższy opis z wymiarowością zmiennej h0\n",
    "h0 = torch.randn(LayerCount, 1, HiddenSize)\n",
    "\n",
    "out, h_t = rnn(data, h0)\n",
    "print(f\"Wyjście warstwy: {out}\")\n",
    "print(f\"Ostatni stan ukryty: {h_t}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę na otrzymane wyjście: ponieważ na wejście sieci podaliśmy sekwencję elementów to dla każdego przetwarzanego elementu sekwencji sieć zwróciła uzyskaną reprezentację ukrytą (skoro mamy 3 neurony w warstwie to taka reprezentacja jest 3-wymiarowa). Dodatkowo zwrócony został ostatni stan ukryty, który moglibyśmy przetwarzać np. kolejnej sieci rekurencyjnej.\n",
    "\n",
    "Sprawdź poprawność uzyskanego powyżej wyniku poprzez uruchomienie sieci rekurencyjnej element po elemencie (wejściem do sieci jest zawsze tylko jeden element sekwencji lub inaczej: sekwencje 1-elementowe). Wypisz na wyjście kolejne otrzymywane wyniki sieci i jej stany ukryte. Uzyskane wyniki powinny być takie same jak te z komórki wyżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:52:59.517892200Z",
     "start_time": "2024-01-10T18:52:59.507193900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyjście sieci: tensor([[[ 0.9886, -0.6947, -0.6778]]], grad_fn=<TransposeBackward1>)\n",
      "Stan ukryty: tensor([[[ 0.9886, -0.6947, -0.6778]]], grad_fn=<StackBackward0>)\n",
      "Wyjście sieci: tensor([[[0.6070, 0.0042, 0.8744]]], grad_fn=<TransposeBackward1>)\n",
      "Stan ukryty: tensor([[[0.6070, 0.0042, 0.8744]]], grad_fn=<StackBackward0>)\n",
      "Wyjście sieci: tensor([[[ 0.9132, -0.7435,  0.9708]]], grad_fn=<TransposeBackward1>)\n",
      "Stan ukryty: tensor([[[ 0.9132, -0.7435,  0.9708]]], grad_fn=<StackBackward0>)\n",
      "Wyjście sieci: tensor([[[ 0.9568, -0.8087,  0.9874]]], grad_fn=<TransposeBackward1>)\n",
      "Stan ukryty: tensor([[[ 0.9568, -0.8087,  0.9874]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h1 = h0\n",
    "for i in range(1, 5):\n",
    "  x_i = torch.FloatTensor([[i, i]]).view(1, 1, 2)\n",
    "  out, h1 = rnn(x_i, h1)\n",
    "  print(f\"Wyjście sieci: {out}\")\n",
    "  print(f\"Stan ukryty: {h1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W prosty sposób można również rozszerzyć naszą warstwę rekurencyjną do sieci dwukierunkowej. W konstruktorze wystarczy ustawić `bidirectional = True`. Jak mówiliśmy na wykładzie taka sieć składa się w rzeczywistości z dwóch sieci - jednej przetwarzającej wyniki od lewej do prawej i drugiej przetwarzającej wyniki od prawej do lewej. W związku z tym należy dla obydwu tych sieci przygotować ich stany początkowe $h_0$, a wymiarowość wyjścia zwiększy się dwukrotnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:52:59.521901600Z",
     "start_time": "2024-01-10T18:52:59.513709500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyjście warstwy:  tensor([[[ 0.7930,  0.2443,  0.7429, -0.0481,  0.3188,  0.3307],\n",
      "         [-0.0417,  0.0146,  0.9826,  0.0224,  0.4474,  0.4442],\n",
      "         [ 0.6254, -0.0239,  0.9966,  0.1991,  0.5588,  0.4567],\n",
      "         [ 0.6265, -0.0590,  0.9996,  0.7583,  0.4821,  0.7634]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "Ostatni stan ukryty:  tensor([[[ 0.6265, -0.0590,  0.9996]],\n",
      "\n",
      "        [[-0.0481,  0.3188,  0.3307]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(FeatureCount, HiddenSize, LayerCount, batch_first=True, bidirectional=True)\n",
    "\n",
    "h0 = torch.randn(LayerCount * 2, 1, HiddenSize)\n",
    "out, h_t = rnn(data, h0)\n",
    "print(\"Wyjście warstwy: \", out)\n",
    "print(\"Ostatni stan ukryty: \", h_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogicznie jak poprzednio uzyskaj wynik z poprzedniej komórki zakładając że na wejściu obiektu `rnn` możesz umieścić jedynie sekwencje jednoelementowe (możesz go jednak wywoływać dowolnie wiele razy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:52:59.532915300Z",
     "start_time": "2024-01-10T18:52:59.519898400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyjście sieci: tensor([[[0.7930, 0.2443, 0.7429, 0.7600, 0.2474, 0.6822]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "Stan ukryty: tensor([[[0.7930, 0.2443, 0.7429]],\n",
      "\n",
      "        [[0.7600, 0.2474, 0.6822]]], grad_fn=<StackBackward0>)\n",
      "Wyjście sieci: tensor([[[-0.0417,  0.0146,  0.9826,  0.2397,  0.4150,  0.3188]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "Stan ukryty: tensor([[[-0.0417,  0.0146,  0.9826]],\n",
      "\n",
      "        [[ 0.2397,  0.4150,  0.3188]]], grad_fn=<StackBackward0>)\n",
      "Wyjście sieci: tensor([[[ 0.6254, -0.0239,  0.9966,  0.0905,  0.4666,  0.4571]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "Stan ukryty: tensor([[[ 0.6254, -0.0239,  0.9966]],\n",
      "\n",
      "        [[ 0.0905,  0.4666,  0.4571]]], grad_fn=<StackBackward0>)\n",
      "Wyjście sieci: tensor([[[ 0.6265, -0.0590,  0.9996, -0.0250,  0.5501,  0.4854]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "Stan ukryty: tensor([[[ 0.6265, -0.0590,  0.9996]],\n",
      "\n",
      "        [[-0.0250,  0.5501,  0.4854]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h1 = h0\n",
    "for i in range(1, 5):\n",
    "  x_i = torch.FloatTensor([[i, i]]).view(1, 1, 2)\n",
    "  out, h1 = rnn(x_i, h1)\n",
    "  print(f\"Wyjście sieci: {out}\")\n",
    "  print(f\"Stan ukryty: {h1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na koniec warto też zauważyć, że samodzielna inicjalizacja stanu wejściowego do sieci rekurencyjnej `h0` nie jest obowiązkowa i na wejście warstwy rekurencyjnej można podać samą sekwencję wejściową. W takim wypadku element `h0` zostanie zainicjalizowany domyślnie wektorem zer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:52:59.609903900Z",
     "start_time": "2024-01-10T18:52:59.528260400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyjście warstwy: tensor([[[ 0.2232,  0.0159,  0.7409, -0.0576,  0.2884,  0.3101],\n",
      "         [ 0.2738, -0.0084,  0.9748, -0.0432,  0.3936,  0.3839],\n",
      "         [ 0.5105, -0.0325,  0.9968,  0.0168,  0.4439,  0.4145],\n",
      "         [ 0.6634, -0.0583,  0.9996,  0.0985,  0.3626,  0.3568]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "Ostatni stan ukryty: tensor([[[ 0.6634, -0.0583,  0.9996]],\n",
      "\n",
      "        [[-0.0576,  0.2884,  0.3101]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out, h_t = rnn(data)\n",
    "print(f\"Wyjście warstwy: {out}\")\n",
    "print(f\"Ostatni stan ukryty: {h_t}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Jakich zmian w kodzie musiałbyś dokonać by uzyskać 3-warstwową sieć rekurencyjną? Jak zmieniłaby się wtedy wymiarowośc `h0` oraz wyjścia sieci neuronowej `h1` i `out`?\n",
    "- W przypadku neuronu rekurencyjnego RNN wejściem do modelu jest sekwencja oraz początkowy/poprzedni stan ukryty `h0`. Korzystając z gotowej implemetancji neuronu LSTM jakiego wejścia się spodziewasz?\n",
    "\n",
    "Odpowiedzi nie musisz zapisywać."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2\n",
    "W ostatnim zadaniu domowym poznawałeś usprawnienia implementacji modeli uczących się w PyTorch poprzez wykorzystanie gotowych elementów z modułu `torch.nn`. Jednak ostatecznie uzyskany przez nas kod nadal przetwarzał pojedyncze instancje tj. nieobsługiwał mini-batchy. Można oczywiście ręcznie zaimplementować indeksowanie, które pozwoli nam na iterowanie po paczkach danych, jednak PyTorch pozwala oczywiście na automatyzację tego procesu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na początek wczytajmy dane uczące z których będziemy korzystać w tym zadaniu. (Może być konieczna instalacja biblioteki `torchtext`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:00.358177100Z",
     "start_time": "2024-01-10T18:52:59.534915400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Confidence', 'in', 'the', 'pound', 'is', 'widely', 'expected', 'to', 'take', 'another', 'sharp', 'dive', 'if', 'trade', 'figures', 'for', 'September', ',', 'due', 'for', 'release', 'tomorrow', ',', 'fail', 'to', 'show', 'a', 'substantial', 'improvement', 'from', 'July', 'and', 'August', \"'s\", 'near-record', 'deficits', '.']\n",
      "['B-NP', 'B-PP', 'B-NP', 'I-NP', 'B-VP', 'I-VP', 'I-VP', 'I-VP', 'I-VP', 'B-NP', 'I-NP', 'I-NP', 'B-SBAR', 'B-NP', 'I-NP', 'B-PP', 'B-NP', 'O', 'B-ADJP', 'B-PP', 'B-NP', 'B-NP', 'O', 'B-VP', 'I-VP', 'I-VP', 'B-NP', 'I-NP', 'I-NP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'B-NP', 'I-NP', 'I-NP', 'O']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.datapipes.iter.sharding import ShardingFilterIterDataPipe\n",
    "from torchtext.datasets import CoNLL2000Chunking\n",
    "\n",
    "train_iter: ShardingFilterIterDataPipe = CoNLL2000Chunking(split='train')\n",
    "\n",
    "sequence_texts = []\n",
    "sequence_tags = []\n",
    "\n",
    "for (texts, _, tags) in train_iter:\n",
    "  sequence_texts.append(texts)\n",
    "  sequence_tags.append(tags)\n",
    "print(sequence_texts[0])\n",
    "print(sequence_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Są to dane z popularnego zbioru CoNLL2000 dotyczące zadania płytkiej analizy frazowej. W liście `seq_texts` umieszczono kolejne zdania, a na liście `seq_tags` umieszczono odpowiadające im zestawy tagów. Zwróć uwagę, że zastosowano tutaj tagowanie BIO np. frazą czasownikową `VP` jest \"is widely expected to take\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszym krokiem przetwarzania jest zamiana słów na ich identyfikatory oraz obsłużenie tokenu `OOV`. W poprzednich zadaniach domowych robiliśmy to ręcznie jednak dzięki bibliotece `torchtext` ten proces równiez możemy zautomatyzować dzięki obiektowi `Vocab` i funkcji `build_vocab_from_iterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:00.441804100Z",
     "start_time": "2024-01-10T18:53:00.358177100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "4403"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator(sequence_texts, specials=[\"<unk>\"], min_freq=5)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja ta na wejście przyjmuje iterator z tekstami, a także opcjonalnie próg `min_freq` pomijający słowa występujące z niewystarczającą częstotliowścią oraz `specials` pozwalający na umieszczenie w słowniku dodatkowych tokenów. Tokeny specjalne domyślnie są umieszczane na początku słownika.\n",
    "\n",
    "Obiekt `Vocab` w prosty sposób zamienia sekwencję tokenów na sekwencję ich indeksów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:00.442309300Z",
     "start_time": "2024-01-10T18:53:00.435054500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[107, 80, 8, 0]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab([\"if\", \"could\", \"in\", \"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oraz na inne proste konwersje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:00.460871500Z",
     "start_time": "2024-01-10T18:53:00.440034500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(vocab.lookup_token(107))\n",
    "print(vocab.lookup_indices([\"<unk>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domyślnie jednak słownik nie obsługuje słów spoza słownika. Dla przykładu \"Confidence\" występuje w rozważanym korpusie mniej niż 2 razy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:00.462870Z",
     "start_time": "2024-01-10T18:53:00.445313300Z"
    }
   },
   "outputs": [],
   "source": [
    "# runtime error wywołany przez out of vocab :D\n",
    "# vocab([\"Confidence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można jednak ustawić domyślny indeks słownika na token `UNK`, który automatycznie zamienia nieznane słowa na indeks tego tokenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:00.463870300Z",
     "start_time": "2024-01-10T18:53:00.448939800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[107, 80, 8, 0, 0]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.set_default_index(0)\n",
    "vocab([\"if\", \"could\", \"in\", \"<unk>\", \"Confidence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następny krokiem jest implementacja własnej klasy zbioru danych, dziedziczącej po klasie `torch.utils.data.Dataset`, która zapewni kompatybilność reprezentacji naszych danych z procedurami m.in. automatycznie tworzącymi batche.\n",
    "\n",
    "Zbiór danych powinien implementować co najmniej 3 funkcje: konstruktor, funkcję zwracającą liczbę elementów w zbiorze `__len__` oraz funckję pozwalającą na dostęp do wybranego elementu zbioru `__getitem__`.\n",
    "\n",
    "Wykorzystując funkcję `build_vocab_from_iterator` uzupełnij implementację poniższej klasy, tak aby funkcja `__getitem__` zwracała elementy zbioru składające się z sekwencji indeksów słów oraz z sekwencji indeksów klas. Należy obsłużyć słowa spoza słownika zamieniająć na token `UNK` słowa występujące jednokrotnie w zbiorze danych. Dodatkowo słownik powinien uwzględniać token specjalny `<PAD>`, najlepiej umieszczony pod indeksem 0, który będzie potrzebny w dalszej części ćwiczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:00.544860500Z",
     "start_time": "2024-01-10T18:53:00.455822800Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "  def __init__(self, sequence_texts: list[list[str]], sequence_tags: list[list[str]]):\n",
    "    self.texts = sequence_texts\n",
    "    self.labels = sequence_tags\n",
    "    self.vocab = build_vocab_from_iterator(\n",
    "      sequence_texts,\n",
    "      specials=[\"<pad>\", \"<unk>\"],\n",
    "      min_freq=2\n",
    "    )\n",
    "    self.vocab.set_default_index(1)\n",
    "    self.tagset = build_vocab_from_iterator(\n",
    "      sequence_tags,\n",
    "      specials=[\"<pad>\"]\n",
    "    )\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    return {\n",
    "      'text': self.vocab(self.texts[item]),\n",
    "      'label': self.tagset(self.labels[item])\n",
    "    }\n",
    "\n",
    "  def get_vocab_size(self):\n",
    "    return len(self.vocab)\n",
    "\n",
    "  def get_tagset_size(self):\n",
    "    return len(self.tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:00.598236600Z",
     "start_time": "2024-01-10T18:53:00.459871700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [1, 9, 3, 1775, 17, 1164, 177, 6, 212, 317, 1216, 6116, 108, 650, 597, 11, 457, 2, 249, 11, 2626, 2963, 2, 4920, 6, 602, 7, 1401, 1752, 21, 732, 8, 543, 10, 8902, 1, 4], 'label': [2, 5, 2, 1, 4, 6, 6, 6, 6, 2, 1, 1, 8, 2, 1, 5, 2, 3, 9, 5, 2, 2, 3, 4, 6, 6, 2, 1, 1, 5, 2, 1, 1, 2, 1, 1, 3]}\n"
     ]
    }
   ],
   "source": [
    "dataset = CoNLLDataset(sequence_texts, sequence_tags)\n",
    "for i in dataset:\n",
    "  print(i)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając zaimplementowany zbiór danych jako obiekt typu `Dataset`, podzielenie go na paczki instancji nie jest trudne. Wystarczy wykorzystać obiekt `DataLoader` i wyspecyfikować w nim rozmiar paczki danych `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:00.599235800Z",
     "start_time": "2024-01-10T18:53:00.569969300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ile utworzono paczek? 2234\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "print(\"Ile utworzono paczek?\", len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niemniej jednak praca z tekstami nie jest niestety taka prosta spróbuj przeiterować pod kolejnych paczkach twojego zbioru:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:00.896094Z",
     "start_time": "2024-01-10T18:53:00.575240700Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;28;43;01mbreak\u001B[39;49;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\2023-anlp\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\PycharmProjects\\2023-anlp\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\PycharmProjects\\2023-anlp\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\2023-anlp\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    205\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001B[39;00m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    263\u001B[0m \u001B[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\2023-anlp\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:127\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMapping):\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 127\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m elem_type(\u001B[43m{\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43md\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43melem\u001B[49m\u001B[43m}\u001B[49m)\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    129\u001B[0m         \u001B[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001B[39;00m\n\u001B[0;32m    130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem}\n",
      "File \u001B[1;32m~\\PycharmProjects\\2023-anlp\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:127\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMapping):\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 127\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m elem_type({key: \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43md\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem})\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    129\u001B[0m         \u001B[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001B[39;00m\n\u001B[0;32m    130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem}\n",
      "File \u001B[1;32m~\\PycharmProjects\\2023-anlp\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:138\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    136\u001B[0m elem_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mnext\u001B[39m(it))\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28mlen\u001B[39m(elem) \u001B[38;5;241m==\u001B[39m elem_size \u001B[38;5;28;01mfor\u001B[39;00m elem \u001B[38;5;129;01min\u001B[39;00m it):\n\u001B[1;32m--> 138\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meach element in list of batch should be of equal size\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    139\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n",
      "\u001B[1;31mRuntimeError\u001B[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "  print(i)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obiekt `DataLoader` próbuje automatycznie podzielić nasze dane na paczki, jednak nie jest to takie proste. Nasze dane składają się z sekwencji o różnych długościach, ciężko jest więc utworzyć z nich eleganckie tensory które mają stałe wymiarowości. Naturalnym rozwiązaniem problemu jest wyznaczenie długości najdłuższej sekwencji i uzupełnienie wszystkich pozostałych sekwencji do tej długości specjlanymi tokenami `<PAD>`. Taka operacja może niestety znacznie zwiększyć wymagania pamięciowe potrzebne do reprezentacji zbioru. Pojawienie się jednej bardzo długiej sekwencji w zbiorze znacznie zwiększa czas przetwarzania dla wszystkich jego elementów. \n",
    "\n",
    "Zauważ, że wymaganiem technicznym nie jest posiadanie całego zbioru danych w postaci jednego dużego tensora, ale stworzenie takiego tensora dla wszystkich sekwnencji w ramach jednej paczki danych. Zwykle długość najdłużej sekwencji w paczce jest dużo mniejsza niż długość najdłuszej sekwencji w całym zbiorze, co pozwoliłoby nam na nie tylko znaczne lepsze wykorzystanie pamięci operacyjnej, ale także na ograniczenie czasu przetwarzania. Z tego powodu w tym zadaniu będziemy dynamicznie tworzyć reprezentację paczki danych -- każda paczka będzie zawierała tyle samo sekwencji, jednak będą one uzupełniane do różnych długości, pod tym względem paczki danych *nie* będą równe.  \n",
    "\n",
    "Do uzupełnienia sekwencji do równych długości przydatna będzie funkcja `pad_sequence`, która również posiada parametr `batch_first` przygotowując dane o odpowiednich wymiarach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:07.497086600Z",
     "start_time": "2024-01-10T18:53:07.475915300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [2, 2],\n",
      "        [3, 0]])\n",
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "data = [torch.tensor([1, 2, 3]), torch.tensor([1, 2])]\n",
    "pad_data = pad_sequence(data, padding_value=0)\n",
    "print(pad_data)\n",
    "\n",
    "pad_data = pad_sequence(data, padding_value=0, batch_first=True)\n",
    "print(pad_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warto zwrócić uwagę, że zrównoleglanie przetwarzania sieci rekurencyjnej nie jest proste, gdyż wyniki przetwarzania zależą od poprzednich wyników. Nie można więc, tak jak w przypadku sieci splotowej, równocześnie obliczyć wyników filtra na wszystkich słowach w tekście, znakomicie zrównoleglając przetwarzanie. Należy najpierw zastosować \"filtr\" na pierwszym słowie, poczekać na wynik, potem na kolejnym itd. W związku z tym najlepszą możliwością zrównoleglenia przetwarzania sieci rekurencyjnych jest obsługa wielu przykładów uczących na raz. Sieć rekurencyjna, inicjalizowana potencjalnie różnymi `h0` dla różnych przykładów uczących, na raz przetwarza pierwsze elementy każdej sekwencji. Następnie przetwarza wszystkie drugie elementy, wszystkie trzecie elementy itd. Przetwarzanie odbywać się będzie po kolei w ramach kolejnych wierszy domyślnej repezentacji `batch_first=False` tj. równocześnie przetwarzane będą wszystkie sekwencje w batchu.\n",
    "\n",
    "Implementacja dynamicznego tworzenia paczek danych (wraz z uzupełnianiem `<PAD>`) jest możliwa dzięki przekazaniu do konstruktora `DataLoader` funkcji `collate_fn`. Funkcja ta na wejście otrzymuje kolekcję danych (w takiej postaci w jakiej zostały one zwrócone z `DataSet` a na wyjście powinna zwrócić tensory reprezentujące paczkę danych. Nasza funkcja `collate_fn` powinna zwrócić 3 tensory. Pierwszy tensor powinien zawierać przykłady uczące (o równej długości), drugi odpowiednio uzupełnione sekwencje tagów oraz trzeci tensor jednowymiarowy (wektor) zawierający długości kolejnych sekwencji w batchu.\n",
    "\n",
    "*UWAGA*: Konwencją umożliwiającą potem efektywniejsze przetwarzanie jest to, że sekwencje w batchu są posortowane ich długościami. Pierwsza sekwencja powinna być najdłuższa, a ostatnia najkrótsza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:09.329281400Z",
     "start_time": "2024-01-10T18:53:09.319438600Z"
    }
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def read(property: str): return operator.itemgetter(property)\n",
    "def collate_fn(batch):\n",
    "  batch = sorted(\n",
    "    batch,\n",
    "    key=lambda x: len(x['text']),\n",
    "    reverse=True\n",
    "  )\n",
    "\n",
    "  texts = map(torch.tensor, map(read('text'), batch))\n",
    "  labels = map(torch.tensor, map(read('label'), batch))\n",
    "  sizes = map(len, map(read('text'), batch))\n",
    "\n",
    "  return (\n",
    "    pad_sequence(texts, padding_value=0, batch_first=True),\n",
    "    pad_sequence(labels, padding_value=0, batch_first=True),\n",
    "    torch.tensor(list(sizes))\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:10.216714200Z",
     "start_time": "2024-01-10T18:53:10.211995700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[   1,    9,    3, 1775,   17, 1164,  177,    6,  212,  317, 1216, 6116,\n",
      "          108,  650,  597,   11,  457,    2,  249,   11, 2626, 2963,    2, 4920,\n",
      "            6,  602,    7, 1401, 1752,   21,  732,    8,  543,   10, 8902,    1,\n",
      "            4],\n",
      "        [ 253,   36,  229,    3,  496,    5,    3,  118,  214, 1190,    6,  197,\n",
      "          977,  172,    6, 1117,   19,   21,   49,  312,  224,   19,  417,    6,\n",
      "         4096,    3, 1775,    2, 1143,    8,  384,  339,   56,  246,  138,    4,\n",
      "            0],\n",
      "        [  55,  246,    1, 2085,  477,   11, 2950,   36,   63, 6154,   25,    3,\n",
      "         5984,   10, 1048,    6, 2788,  113,   69,  260, 2043,    9,   64, 4593,\n",
      "          225, 3267,   88,  817,    4,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [2263,    5,    3, 4530, 4614, 1704,   10, 4293, 2543,    6,    7,  228,\n",
      "         1314,  260,   36,  794,    6, 1916,    7,    1,    9, 2950,   96,    3,\n",
      "          230,  126,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0]]), tensor([[2, 5, 2, 1, 4, 6, 6, 6, 6, 2, 1, 1, 8, 2, 1, 5, 2, 3, 9, 5, 2, 2, 3, 4,\n",
      "         6, 6, 2, 1, 1, 5, 2, 1, 1, 2, 1, 1, 3],\n",
      "        [2, 4, 6, 2, 1, 5, 2, 1, 4, 6, 6, 6, 2, 1, 5, 2, 1, 5, 2, 1, 1, 1, 1, 4,\n",
      "         6, 2, 1, 3, 2, 3, 2, 1, 1, 1, 4, 3, 0],\n",
      "        [3, 2, 4, 2, 1, 5, 2, 4, 6, 6, 5, 2, 1, 2, 1, 4, 6, 2, 1, 1, 1, 5, 2, 1,\n",
      "         1, 1, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [3, 5, 2, 1, 2, 1, 2, 1, 1, 5, 2, 1, 1, 1, 4, 6, 6, 6, 2, 1, 5, 2, 5, 2,\n",
      "         1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), tensor([37, 36, 29, 27]))\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, collate_fn=collate_fn)\n",
    "for i in dataloader:\n",
    "  print(i)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uzupełnienie sekwencji tokenami `<PAD>` do pełnej długości wydaje sie być dobrym pomysłem i pozwala na operowanie na tensorach które łatwo przesłać na kartę graficzną czy wykonać równoległe obliczenia na wszystkich jego elementach. Niemniej jednak fakt, że nasze sekwencje nie są równiej długości jest nadal niezwykle istotny przy wielu różnych obliczeniach. W szczególności jest on istotny przy przetwarzaniu sekwencji przez sieć rekurencyjną. W przypadku sieci rekurencyjnej przetwarzającej sekwencję uzupełnioną do końca zerami, sieć nie zwróci reprezentacji dla każdego słowa ale dla wszystkich elementów sekwencji czyli także dla końcowych elementów-zer. Ewidentnie sieć wykonała wiele niepotrzebnych operacji obliczeniowych tracąc czas (a przecież sieci rekurencyjne do najszybszych nie należą) jednakże prawidłowy wynik nadal jest do odzyskania. Możemy przecież post-factum wziąć pod uwagę tylko (długość sekwencji)-pierwszych elementów wyniku, uzyskując taki sam wynik jak przy przetwarzaniu sekwencji bez uzupełnienia.\n",
    "\n",
    "Tak się jednak nie dzieje, gdy rozważamy dwukierunkową sieć rekurencyjną. Sieć iterująca w tył rozpoczęła przetwarzanie od początkowego stanu ukrytego, a następnie akutalizowała go za kolejne wejścia uważając sekwencję tokenów `<PAD>`! Wynik obliczeń tej sieci jest więc różny niż dla sekwencji nieuzupełnionej i co więcej nie jest on do odzyskania. Uzupełniając sekwnecję np. zerami nie tylko wykonujemy nadmiarowe obliczenia, ale także zaburzamy wyniki.\n",
    "\n",
    "Na szczęście sieci rekurencyjne zaimplementowane w PyTorch obsługją także specjalny format danych tzw. `PackedSequence`, który przechowuje informację o długościach sekwencji zapewniając oszczędność obliczeń i takie same wyniki jak dla sekwencji bez uzupełniania. Przed wykonaniem obliczeń siecią rekurencyjną możemy dane zapakować do tego formatu, a następnie przetworzony wynik możemy z powrotem odpakować do postaci uzupełnionego do pełnej długości tensora. Pomocne są w tym dwie funkcje: `pack_padded_sequence` pakująca dane do tego formatu oraz funkcja odpakowująca `pad_packed_sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:12.185038300Z",
     "start_time": "2024-01-10T18:53:12.170887100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([ 1, -1, 10,  2, -2, 20,  3, -3,  4,  5]), batch_sizes=tensor([3, 3, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "data = torch.tensor([[1, 2, 3, 4, 5], [-1, -2, -3, 0, 0], [10, 20, 0, 0, 0]])\n",
    "lengths = torch.tensor([5, 3, 2])\n",
    "\n",
    "packed_data = pack_padded_sequence(data, lengths, batch_first=True)\n",
    "print(packed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapakowaliśmy 3 sekwencje, które uzupełnione były do długości 5. Ponieważ kolejne sekwencje są umieszczone w kolejnych wierszach tensora należy wyspecyfikować argument `batch_first=True`. Format `PackedSequence` przechowuje tensor `data` oraz tensor `batch_sizes`. Tak jak opisywaliśmy, sieć rekurencyjna zrównolegla swoje działania poprzez jednoczene przetwarzanie wszystkich sekwencji na raz, iterując po ich kolejnych elementach. Z tego powodu `batch_sizes` opisują wielkość przetwarzanej paczki danych (tj. liczba przetwarzanych sekwencji) w każdej iteracji. Na początku przetwarzamy 3 sekwencje, potem znowu 3, a następnie tylko dwie, gdyż ostatnia sekwencja miała tylko 2 elementy. Prześledź, że właśnie w taki sposób zostały ułożone dane w `data`.\n",
    "\n",
    "Porównajmy działanie neuronu rekurencyjnego dla danych uzupełnionych tokenami `<PAD>` oraz dla danych spakowanych do `PackedSequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:13.556052700Z",
     "start_time": "2024-01-10T18:53:13.546612400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyjście sieci dla danych o stałej długości:  tensor([[[ 0.0302],\n",
      "         [-0.7016],\n",
      "         [-0.9277],\n",
      "         [-0.9866],\n",
      "         [-0.9977]],\n",
      "\n",
      "        [[ 0.9490],\n",
      "         [ 0.9880],\n",
      "         [ 0.9980],\n",
      "         [ 0.6390],\n",
      "         [ 0.6736]],\n",
      "\n",
      "        [[-1.0000],\n",
      "         [-1.0000],\n",
      "         [ 0.7988],\n",
      "         [ 0.6585],\n",
      "         [ 0.6718]]], grad_fn=<TransposeBackward1>)\n",
      "Wyjście sieci dla danych spakowanych:  PackedSequence(data=tensor([[ 0.0302],\n",
      "        [ 0.9490],\n",
      "        [-1.0000],\n",
      "        [-0.7016],\n",
      "        [ 0.9880],\n",
      "        [-1.0000],\n",
      "        [-0.9277],\n",
      "        [ 0.9980],\n",
      "        [-0.9866],\n",
      "        [-0.9977]], grad_fn=<CatBackward0>), batch_sizes=tensor([3, 3, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n",
      "Wyjście sieci po rozpakowaniu:  tensor([[[ 0.0302],\n",
      "         [-0.7016],\n",
      "         [-0.9277],\n",
      "         [-0.9866],\n",
      "         [-0.9977]],\n",
      "\n",
      "        [[ 0.9490],\n",
      "         [ 0.9880],\n",
      "         [ 0.9980],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-1.0000],\n",
      "         [-1.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "FeatureCount = 1\n",
    "LayerCount = 1\n",
    "HiddenSize = 1\n",
    "\n",
    "data = torch.FloatTensor([[1, 2, 3, 4, 5], [-1, -2, -3, 0, 0], [10, 20, 0, 0, 0]]).view(3, 5, 1)\n",
    "lengths = torch.tensor([5, 3, 2])\n",
    "\n",
    "rnn = nn.RNN(FeatureCount, HiddenSize, LayerCount, batch_first=True)\n",
    "out, _ = rnn(data)\n",
    "print(\"Wyjście sieci dla danych o stałej długości: \", out)\n",
    "\n",
    "packed_data = pack_padded_sequence(data, lengths, batch_first=True)\n",
    "out_packed, _ = rnn(packed_data)\n",
    "print(\"Wyjście sieci dla danych spakowanych: \", out_packed)\n",
    "out, out_len = pad_packed_sequence(out_packed, batch_first=True)\n",
    "print(\"Wyjście sieci po rozpakowaniu: \", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę jak ułożone są spakowane wyniki sieci.\n",
    "\n",
    "Stworzyliśmy zbiór danych, mamy także zaimplementowany mechanizm dynamicznych paczek danych, a także wiemy jak efektywnie wykorzystywać je do obliczeń siecami rekurencyjnymi. Połóżmy więc wisienkę na torcie i stwórzmy model do tagowania tych danych. Model powinien składać się z warstwy zanurzeń słów, która jest wejściem do jednowarstwowego, dwukierunkowego LSTM. Ostatecznie predykcja jest wykonywana przez warstwę liniową (softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:53:15.151491500Z",
     "start_time": "2024-01-10T18:53:15.141553400Z"
    }
   },
   "outputs": [],
   "source": [
    "WordEmbedding = 20\n",
    "\n",
    "class TaggerNet(nn.Module):\n",
    "  def __init__(self, vocab_size, hidden_size, tag_count):\n",
    "    \"\"\" Argumentami konstruktora jest rozmiar słownika, \n",
    "        liczba neuronów w warstwie rekurencyjnej \n",
    "        oraz liczba klas/tagów\n",
    "    \"\"\"\n",
    "    super(TaggerNet, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, WordEmbedding)\n",
    "    self.lstm = nn.LSTM(WordEmbedding, hidden_size, bidirectional=True)\n",
    "    self.linear = nn.Linear(hidden_size * 2, tag_count)\n",
    "    self.tag_count = tag_count\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, sentence, sequence_lengths):\n",
    "    \"\"\" Wejściem jest paczka zdań (słowa reprezentowane przez indeksy)\n",
    "        oraz tensor ich długości \n",
    "    \"\"\"\n",
    "    x = self.embedding(sentence)\n",
    "    x = pack_padded_sequence(x, sequence_lengths, batch_first=True)\n",
    "    x, _ = self.lstm(x)\n",
    "    x, _ = pad_packed_sequence(x, batch_first=True)\n",
    "    x = self.linear(x)\n",
    "    x = self.softmax(x)\n",
    "    return x.view(len(sentence), self.tag_count, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplemetuj finalną pętlę uczącą model. \n",
    "- Powinieneś wykorzystać błąd entropii krzyżowej jako funkcję straty policzoną dla każdego przewidzianego taga. \n",
    "- Zwróć uwagę, że wyniki sieci są w postaci rozpakowanej tj. uzupełnionej zerami do pełnej długości. Błąd nie powinien być liczony dla tych predykcji! Możesz to osiągnąć np. sprytnie wykorzystując parametr `ignore_index` klasy `nn.CrossEntropyLoss`. \n",
    "- Jako algorytm optymalizacyjny wykorzystaj `torch.optim.Adam`.\n",
    "- Jedną z technik stabilizujących trening sieci jest przycinanie gradientu. W PyTorch możesz to uzyskać wywołując pomiędzy obliczeniem gradientu a wywołaniem kroku optymalizatora funkcji `nn.utils.clip_grad_norm_(model.parameters(), TRESHOLD)`. Zwróć uwagę, że nazwa funkcji zakończona jest `_` to znaczy, że operacja jest wykonywana in-place i wyniku funkcji nie trzeba podstawiać do żadnej zmiennej.\n",
    "- Celem ćwiczenia nie jest wybór hiperparametrów, ani uzyskiwanie wysokich trafności."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:54:10.472190400Z",
     "start_time": "2024-01-10T18:53:16.535385600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0070)\n",
      "tensor(0.0140)\n",
      "tensor(0.0210)\n",
      "tensor(0.0281)\n",
      "tensor(0.0351)\n",
      "tensor(0.0421)\n",
      "tensor(0.0491)\n",
      "tensor(0.0561)\n",
      "tensor(0.0631)\n",
      "tensor(0.0701)\n",
      "tensor(0.0771)\n",
      "tensor(0.0842)\n",
      "tensor(0.0912)\n",
      "tensor(0.0982)\n",
      "tensor(0.1052)\n",
      "tensor(0.1122)\n",
      "tensor(0.1192)\n",
      "tensor(0.1262)\n",
      "tensor(0.1332)\n",
      "tensor(0.1402)\n",
      "tensor(0.1472)\n",
      "tensor(0.1541)\n",
      "tensor(0.1611)\n",
      "tensor(0.1681)\n",
      "tensor(0.1751)\n",
      "tensor(0.1820)\n",
      "tensor(0.1890)\n",
      "tensor(0.1960)\n",
      "tensor(0.2030)\n",
      "tensor(0.2099)\n",
      "tensor(0.2169)\n",
      "tensor(0.2238)\n",
      "tensor(0.2308)\n",
      "tensor(0.2377)\n",
      "tensor(0.2447)\n",
      "tensor(0.2516)\n",
      "tensor(0.2585)\n",
      "tensor(0.2655)\n",
      "tensor(0.2724)\n",
      "tensor(0.2794)\n",
      "tensor(0.2863)\n",
      "tensor(0.2932)\n",
      "tensor(0.3001)\n",
      "tensor(0.3070)\n",
      "tensor(0.3139)\n",
      "tensor(0.3208)\n",
      "tensor(0.3277)\n",
      "tensor(0.3346)\n",
      "tensor(0.3415)\n",
      "tensor(0.3484)\n",
      "tensor(0.3553)\n",
      "tensor(0.3622)\n",
      "tensor(0.3691)\n",
      "tensor(0.3759)\n",
      "tensor(0.3828)\n",
      "tensor(0.3896)\n",
      "tensor(0.3965)\n",
      "tensor(0.4033)\n",
      "tensor(0.4103)\n",
      "tensor(0.4171)\n",
      "tensor(0.4240)\n",
      "tensor(0.4309)\n",
      "tensor(0.4377)\n",
      "tensor(0.4445)\n",
      "tensor(0.4513)\n",
      "tensor(0.4581)\n",
      "tensor(0.4650)\n",
      "tensor(0.4717)\n",
      "tensor(0.4785)\n",
      "tensor(0.4854)\n",
      "tensor(0.4921)\n",
      "tensor(0.4989)\n",
      "tensor(0.5058)\n",
      "tensor(0.5126)\n",
      "tensor(0.5194)\n",
      "tensor(0.5262)\n",
      "tensor(0.5331)\n",
      "tensor(0.5399)\n",
      "tensor(0.5467)\n",
      "tensor(0.5536)\n",
      "tensor(0.5605)\n",
      "tensor(0.5674)\n",
      "tensor(0.5742)\n",
      "tensor(0.5810)\n",
      "tensor(0.5878)\n",
      "tensor(0.5946)\n",
      "tensor(0.6014)\n",
      "tensor(0.6082)\n",
      "tensor(0.6149)\n",
      "tensor(0.6217)\n",
      "tensor(0.6284)\n",
      "tensor(0.6352)\n",
      "tensor(0.6419)\n",
      "tensor(0.6486)\n",
      "tensor(0.6554)\n",
      "tensor(0.6621)\n",
      "tensor(0.6689)\n",
      "tensor(0.6757)\n",
      "tensor(0.6825)\n",
      "tensor(0.6892)\n",
      "tensor(0.6959)\n",
      "tensor(0.7027)\n",
      "tensor(0.7094)\n",
      "tensor(0.7161)\n",
      "tensor(0.7228)\n",
      "tensor(0.7295)\n",
      "tensor(0.7362)\n",
      "tensor(0.7430)\n",
      "tensor(0.7498)\n",
      "tensor(0.7565)\n",
      "tensor(0.7632)\n",
      "tensor(0.7700)\n",
      "tensor(0.7767)\n",
      "tensor(0.7834)\n",
      "tensor(0.7901)\n",
      "tensor(0.7968)\n",
      "tensor(0.8035)\n",
      "tensor(0.8103)\n",
      "tensor(0.8169)\n",
      "tensor(0.8235)\n",
      "tensor(0.8303)\n",
      "tensor(0.8369)\n",
      "tensor(0.8437)\n",
      "tensor(0.8504)\n",
      "tensor(0.8572)\n",
      "tensor(0.8639)\n",
      "tensor(0.8706)\n",
      "tensor(0.8773)\n",
      "tensor(0.8839)\n",
      "tensor(0.8907)\n",
      "tensor(0.8975)\n",
      "tensor(0.9043)\n",
      "tensor(0.9110)\n",
      "tensor(0.9177)\n",
      "tensor(0.9245)\n",
      "tensor(0.9313)\n",
      "tensor(0.9380)\n",
      "tensor(0.9449)\n",
      "tensor(0.9517)\n",
      "tensor(0.9584)\n",
      "tensor(0.9652)\n",
      "tensor(0.9720)\n",
      "tensor(0.9787)\n",
      "tensor(0.9854)\n",
      "tensor(0.9921)\n",
      "tensor(0.9988)\n",
      "tensor(1.0055)\n",
      "tensor(1.0123)\n",
      "tensor(1.0192)\n",
      "tensor(1.0260)\n",
      "tensor(1.0327)\n",
      "tensor(1.0394)\n",
      "tensor(1.0461)\n",
      "tensor(1.0528)\n",
      "tensor(1.0595)\n",
      "tensor(1.0662)\n",
      "tensor(1.0729)\n",
      "tensor(1.0796)\n",
      "tensor(1.0864)\n",
      "tensor(1.0933)\n",
      "tensor(1.1000)\n",
      "tensor(1.1068)\n",
      "tensor(1.1134)\n",
      "tensor(1.1203)\n",
      "tensor(1.1271)\n",
      "tensor(1.1340)\n",
      "tensor(1.1408)\n",
      "tensor(1.1475)\n",
      "tensor(1.1542)\n",
      "tensor(1.1610)\n",
      "tensor(1.1678)\n",
      "tensor(1.1745)\n",
      "tensor(1.1814)\n",
      "tensor(1.1880)\n",
      "tensor(1.1948)\n",
      "tensor(1.2015)\n",
      "tensor(1.2082)\n",
      "tensor(1.2148)\n",
      "tensor(1.2216)\n",
      "tensor(1.2284)\n",
      "tensor(1.2351)\n",
      "tensor(1.2418)\n",
      "tensor(1.2486)\n",
      "tensor(1.2553)\n",
      "tensor(1.2620)\n",
      "tensor(1.2687)\n",
      "tensor(1.2754)\n",
      "tensor(1.2820)\n",
      "tensor(1.2886)\n",
      "tensor(1.2952)\n",
      "tensor(1.3019)\n",
      "tensor(1.3084)\n",
      "tensor(1.3152)\n",
      "tensor(1.3219)\n",
      "tensor(1.3289)\n",
      "tensor(1.3358)\n",
      "tensor(1.3424)\n",
      "tensor(1.3491)\n",
      "tensor(1.3560)\n",
      "tensor(1.3628)\n",
      "tensor(1.3695)\n",
      "tensor(1.3762)\n",
      "tensor(1.3828)\n",
      "tensor(1.3896)\n",
      "tensor(1.3963)\n",
      "tensor(1.4030)\n",
      "tensor(1.4098)\n",
      "tensor(1.4164)\n",
      "tensor(1.4232)\n",
      "tensor(1.4299)\n",
      "tensor(1.4366)\n",
      "tensor(1.4435)\n",
      "tensor(1.4501)\n",
      "tensor(1.4568)\n",
      "tensor(1.4634)\n",
      "tensor(1.4702)\n",
      "tensor(1.4769)\n",
      "tensor(1.4837)\n",
      "tensor(1.4903)\n",
      "tensor(1.4970)\n",
      "tensor(1.5037)\n",
      "tensor(1.5103)\n",
      "tensor(1.5171)\n",
      "tensor(1.5237)\n",
      "tensor(1.5306)\n",
      "tensor(1.5372)\n",
      "tensor(1.5440)\n",
      "tensor(1.5507)\n",
      "tensor(1.5573)\n",
      "tensor(1.5641)\n",
      "tensor(1.5710)\n",
      "tensor(1.5778)\n",
      "tensor(1.5845)\n",
      "tensor(1.5911)\n",
      "tensor(1.5978)\n",
      "tensor(1.6044)\n",
      "tensor(1.6111)\n",
      "tensor(1.6180)\n",
      "tensor(1.6250)\n",
      "tensor(1.6316)\n",
      "tensor(1.6385)\n",
      "tensor(1.6452)\n",
      "tensor(1.6519)\n",
      "tensor(1.6585)\n",
      "tensor(1.6652)\n",
      "tensor(1.6719)\n",
      "tensor(1.6786)\n",
      "tensor(1.6853)\n",
      "tensor(1.6920)\n",
      "tensor(1.6988)\n",
      "tensor(1.7055)\n",
      "tensor(1.7122)\n",
      "tensor(1.7188)\n",
      "tensor(1.7255)\n",
      "tensor(1.7321)\n",
      "tensor(1.7387)\n",
      "tensor(1.7454)\n",
      "tensor(1.7520)\n",
      "tensor(1.7587)\n",
      "tensor(1.7655)\n",
      "tensor(1.7722)\n",
      "tensor(1.7788)\n",
      "tensor(1.7854)\n",
      "tensor(1.7921)\n",
      "tensor(1.7988)\n",
      "tensor(1.8055)\n",
      "tensor(1.8121)\n",
      "tensor(1.8187)\n",
      "tensor(1.8253)\n",
      "tensor(1.8322)\n",
      "tensor(1.8390)\n",
      "tensor(1.8458)\n",
      "tensor(1.8524)\n",
      "tensor(1.8590)\n",
      "tensor(1.8655)\n",
      "tensor(1.8722)\n",
      "tensor(1.8789)\n",
      "tensor(1.8855)\n",
      "tensor(1.8921)\n",
      "tensor(1.8988)\n",
      "tensor(1.9055)\n",
      "tensor(1.9121)\n",
      "tensor(1.9188)\n",
      "tensor(1.9255)\n",
      "tensor(1.9322)\n",
      "tensor(1.9389)\n",
      "tensor(1.9455)\n",
      "tensor(1.9522)\n",
      "tensor(1.9589)\n",
      "tensor(1.9658)\n",
      "tensor(1.9726)\n",
      "tensor(1.9793)\n",
      "tensor(1.9861)\n",
      "tensor(1.9928)\n",
      "tensor(1.9995)\n",
      "tensor(2.0064)\n",
      "tensor(2.0131)\n",
      "tensor(2.0198)\n",
      "tensor(2.0263)\n",
      "tensor(2.0330)\n",
      "tensor(2.0396)\n",
      "tensor(2.0461)\n",
      "tensor(2.0527)\n",
      "tensor(2.0594)\n",
      "tensor(2.0661)\n",
      "tensor(2.0727)\n",
      "tensor(2.0794)\n",
      "tensor(2.0862)\n",
      "tensor(2.0930)\n",
      "tensor(2.0997)\n",
      "tensor(2.1063)\n",
      "tensor(2.1130)\n",
      "tensor(2.1197)\n",
      "tensor(2.1264)\n",
      "tensor(2.1332)\n",
      "tensor(2.1399)\n",
      "tensor(2.1465)\n",
      "tensor(2.1530)\n",
      "tensor(2.1599)\n",
      "tensor(2.1666)\n",
      "tensor(2.1732)\n",
      "tensor(2.1800)\n",
      "tensor(2.1867)\n",
      "tensor(2.1933)\n",
      "tensor(2.2000)\n",
      "tensor(2.2068)\n",
      "tensor(2.2136)\n",
      "tensor(2.2202)\n",
      "tensor(2.2272)\n",
      "tensor(2.2339)\n",
      "tensor(2.2407)\n",
      "tensor(2.2476)\n",
      "tensor(2.2546)\n",
      "tensor(2.2612)\n",
      "tensor(2.2680)\n",
      "tensor(2.2747)\n",
      "tensor(2.2814)\n",
      "tensor(2.2881)\n",
      "tensor(2.2949)\n",
      "tensor(2.3016)\n",
      "tensor(2.3082)\n",
      "tensor(2.3148)\n",
      "tensor(2.3215)\n",
      "tensor(2.3282)\n",
      "tensor(2.3348)\n",
      "tensor(2.3415)\n",
      "tensor(2.3482)\n",
      "tensor(2.3549)\n",
      "tensor(2.3616)\n",
      "tensor(2.3683)\n",
      "tensor(2.3753)\n",
      "tensor(2.3820)\n",
      "tensor(2.3886)\n",
      "tensor(2.3954)\n",
      "tensor(2.4020)\n",
      "tensor(2.4086)\n",
      "tensor(2.4153)\n",
      "tensor(2.4220)\n",
      "tensor(2.4288)\n",
      "tensor(2.4354)\n",
      "tensor(2.4422)\n",
      "tensor(2.4489)\n",
      "tensor(2.4556)\n",
      "tensor(2.4623)\n",
      "tensor(2.4690)\n",
      "tensor(2.4756)\n",
      "tensor(2.4823)\n",
      "tensor(2.4891)\n",
      "tensor(2.4958)\n",
      "tensor(2.5026)\n",
      "tensor(2.5092)\n",
      "tensor(2.5160)\n",
      "tensor(2.5227)\n",
      "tensor(2.5294)\n",
      "tensor(2.5360)\n",
      "tensor(2.5428)\n",
      "tensor(2.5494)\n",
      "tensor(2.5560)\n",
      "tensor(2.5627)\n",
      "tensor(2.5694)\n",
      "tensor(2.5760)\n",
      "tensor(2.5826)\n",
      "tensor(2.5893)\n",
      "tensor(2.5960)\n",
      "tensor(2.6027)\n",
      "tensor(2.6094)\n",
      "tensor(2.6162)\n",
      "tensor(2.6229)\n",
      "tensor(2.6296)\n",
      "tensor(2.6363)\n",
      "tensor(2.6432)\n",
      "tensor(2.6498)\n",
      "tensor(2.6567)\n",
      "tensor(2.6633)\n",
      "tensor(2.6700)\n",
      "tensor(2.6768)\n",
      "tensor(2.6834)\n",
      "tensor(2.6902)\n",
      "tensor(2.6969)\n",
      "tensor(2.7036)\n",
      "tensor(2.7102)\n",
      "tensor(2.7169)\n",
      "tensor(2.7236)\n",
      "tensor(2.7302)\n",
      "tensor(2.7368)\n",
      "tensor(2.7433)\n",
      "tensor(2.7500)\n",
      "tensor(2.7567)\n",
      "tensor(2.7634)\n",
      "tensor(2.7700)\n",
      "tensor(2.7765)\n",
      "tensor(2.7831)\n",
      "tensor(2.7897)\n",
      "tensor(2.7963)\n",
      "tensor(2.8031)\n",
      "tensor(2.8098)\n",
      "tensor(2.8165)\n",
      "tensor(2.8232)\n",
      "tensor(2.8299)\n",
      "tensor(2.8366)\n",
      "tensor(2.8433)\n",
      "tensor(2.8499)\n",
      "tensor(2.8566)\n",
      "tensor(2.8632)\n",
      "tensor(2.8699)\n",
      "tensor(2.8765)\n",
      "tensor(2.8833)\n",
      "tensor(2.8900)\n",
      "tensor(2.8967)\n",
      "tensor(2.9033)\n",
      "tensor(2.9099)\n",
      "tensor(2.9168)\n",
      "tensor(2.9233)\n",
      "tensor(2.9301)\n",
      "tensor(2.9366)\n",
      "tensor(2.9434)\n",
      "tensor(2.9501)\n",
      "tensor(2.9568)\n",
      "tensor(2.9637)\n",
      "tensor(2.9703)\n",
      "tensor(2.9773)\n",
      "tensor(2.9839)\n",
      "tensor(2.9905)\n",
      "tensor(2.9971)\n",
      "tensor(3.0038)\n",
      "tensor(3.0106)\n",
      "tensor(3.0172)\n",
      "0\n",
      "tensor(0.0066)\n",
      "tensor(0.0133)\n",
      "tensor(0.0199)\n",
      "tensor(0.0266)\n",
      "tensor(0.0333)\n",
      "tensor(0.0399)\n",
      "tensor(0.0466)\n",
      "tensor(0.0533)\n",
      "tensor(0.0601)\n",
      "tensor(0.0668)\n",
      "tensor(0.0735)\n",
      "tensor(0.0802)\n",
      "tensor(0.0869)\n",
      "tensor(0.0935)\n",
      "tensor(0.1001)\n",
      "tensor(0.1068)\n",
      "tensor(0.1135)\n",
      "tensor(0.1202)\n",
      "tensor(0.1268)\n",
      "tensor(0.1335)\n",
      "tensor(0.1401)\n",
      "tensor(0.1469)\n",
      "tensor(0.1535)\n",
      "tensor(0.1600)\n",
      "tensor(0.1667)\n",
      "tensor(0.1734)\n",
      "tensor(0.1802)\n",
      "tensor(0.1867)\n",
      "tensor(0.1933)\n",
      "tensor(0.1999)\n",
      "tensor(0.2066)\n",
      "tensor(0.2132)\n",
      "tensor(0.2198)\n",
      "tensor(0.2265)\n",
      "tensor(0.2331)\n",
      "tensor(0.2399)\n",
      "tensor(0.2466)\n",
      "tensor(0.2533)\n",
      "tensor(0.2599)\n",
      "tensor(0.2667)\n",
      "tensor(0.2733)\n",
      "tensor(0.2799)\n",
      "tensor(0.2866)\n",
      "tensor(0.2933)\n",
      "tensor(0.3000)\n",
      "tensor(0.3068)\n",
      "tensor(0.3136)\n",
      "tensor(0.3202)\n",
      "tensor(0.3270)\n",
      "tensor(0.3336)\n",
      "tensor(0.3403)\n",
      "tensor(0.3471)\n",
      "tensor(0.3538)\n",
      "tensor(0.3606)\n",
      "tensor(0.3673)\n",
      "tensor(0.3739)\n",
      "tensor(0.3806)\n",
      "tensor(0.3874)\n",
      "tensor(0.3943)\n",
      "tensor(0.4011)\n",
      "tensor(0.4078)\n",
      "tensor(0.4146)\n",
      "tensor(0.4213)\n",
      "tensor(0.4280)\n",
      "tensor(0.4346)\n",
      "tensor(0.4415)\n",
      "tensor(0.4482)\n",
      "tensor(0.4549)\n",
      "tensor(0.4615)\n",
      "tensor(0.4683)\n",
      "tensor(0.4750)\n",
      "tensor(0.4817)\n",
      "tensor(0.4886)\n",
      "tensor(0.4953)\n",
      "tensor(0.5020)\n",
      "tensor(0.5088)\n",
      "tensor(0.5158)\n",
      "tensor(0.5225)\n",
      "tensor(0.5293)\n",
      "tensor(0.5363)\n",
      "tensor(0.5432)\n",
      "tensor(0.5501)\n",
      "tensor(0.5568)\n",
      "tensor(0.5637)\n",
      "tensor(0.5706)\n",
      "tensor(0.5774)\n",
      "tensor(0.5840)\n",
      "tensor(0.5906)\n",
      "tensor(0.5972)\n",
      "tensor(0.6039)\n",
      "tensor(0.6106)\n",
      "tensor(0.6172)\n",
      "tensor(0.6239)\n",
      "tensor(0.6305)\n",
      "tensor(0.6372)\n",
      "tensor(0.6440)\n",
      "tensor(0.6507)\n",
      "tensor(0.6574)\n",
      "tensor(0.6641)\n",
      "tensor(0.6707)\n",
      "tensor(0.6773)\n",
      "tensor(0.6840)\n",
      "tensor(0.6907)\n",
      "tensor(0.6974)\n",
      "tensor(0.7041)\n",
      "tensor(0.7107)\n",
      "tensor(0.7173)\n",
      "tensor(0.7241)\n",
      "tensor(0.7308)\n",
      "tensor(0.7375)\n",
      "tensor(0.7441)\n",
      "tensor(0.7509)\n",
      "tensor(0.7576)\n",
      "tensor(0.7643)\n",
      "tensor(0.7710)\n",
      "tensor(0.7775)\n",
      "tensor(0.7842)\n",
      "tensor(0.7909)\n",
      "tensor(0.7974)\n",
      "tensor(0.8038)\n",
      "tensor(0.8106)\n",
      "tensor(0.8171)\n",
      "tensor(0.8238)\n",
      "tensor(0.8305)\n",
      "tensor(0.8373)\n",
      "tensor(0.8440)\n",
      "tensor(0.8506)\n",
      "tensor(0.8573)\n",
      "tensor(0.8639)\n",
      "tensor(0.8706)\n",
      "tensor(0.8773)\n",
      "tensor(0.8841)\n",
      "tensor(0.8908)\n",
      "tensor(0.8975)\n",
      "tensor(0.9042)\n",
      "tensor(0.9111)\n",
      "tensor(0.9178)\n",
      "tensor(0.9246)\n",
      "tensor(0.9314)\n",
      "tensor(0.9382)\n",
      "tensor(0.9451)\n",
      "tensor(0.9518)\n",
      "tensor(0.9584)\n",
      "tensor(0.9650)\n",
      "tensor(0.9717)\n",
      "tensor(0.9783)\n",
      "tensor(0.9848)\n",
      "tensor(0.9917)\n",
      "tensor(0.9986)\n",
      "tensor(1.0053)\n",
      "tensor(1.0120)\n",
      "tensor(1.0187)\n",
      "tensor(1.0254)\n",
      "tensor(1.0320)\n",
      "tensor(1.0386)\n",
      "tensor(1.0453)\n",
      "tensor(1.0520)\n",
      "tensor(1.0587)\n",
      "tensor(1.0655)\n",
      "tensor(1.0723)\n",
      "tensor(1.0790)\n",
      "tensor(1.0858)\n",
      "tensor(1.0925)\n",
      "tensor(1.0993)\n",
      "tensor(1.1061)\n",
      "tensor(1.1130)\n",
      "tensor(1.1198)\n",
      "tensor(1.1265)\n",
      "tensor(1.1331)\n",
      "tensor(1.1399)\n",
      "tensor(1.1467)\n",
      "tensor(1.1533)\n",
      "tensor(1.1602)\n",
      "tensor(1.1669)\n",
      "tensor(1.1736)\n",
      "tensor(1.1803)\n",
      "tensor(1.1870)\n",
      "tensor(1.1936)\n",
      "tensor(1.2004)\n",
      "tensor(1.2071)\n",
      "tensor(1.2138)\n",
      "tensor(1.2205)\n",
      "tensor(1.2272)\n",
      "tensor(1.2340)\n",
      "tensor(1.2407)\n",
      "tensor(1.2474)\n",
      "tensor(1.2540)\n",
      "tensor(1.2607)\n",
      "tensor(1.2673)\n",
      "tensor(1.2739)\n",
      "tensor(1.2805)\n",
      "tensor(1.2869)\n",
      "tensor(1.2937)\n",
      "tensor(1.3003)\n",
      "tensor(1.3073)\n",
      "tensor(1.3141)\n",
      "tensor(1.3208)\n",
      "tensor(1.3274)\n",
      "tensor(1.3344)\n",
      "tensor(1.3412)\n",
      "tensor(1.3479)\n",
      "tensor(1.3546)\n",
      "tensor(1.3612)\n",
      "tensor(1.3679)\n",
      "tensor(1.3747)\n",
      "tensor(1.3813)\n",
      "tensor(1.3881)\n",
      "tensor(1.3947)\n",
      "tensor(1.4014)\n",
      "tensor(1.4082)\n",
      "tensor(1.4149)\n",
      "tensor(1.4218)\n",
      "tensor(1.4284)\n",
      "tensor(1.4351)\n",
      "tensor(1.4417)\n",
      "tensor(1.4484)\n",
      "tensor(1.4551)\n",
      "tensor(1.4618)\n",
      "tensor(1.4683)\n",
      "tensor(1.4750)\n",
      "tensor(1.4816)\n",
      "tensor(1.4882)\n",
      "tensor(1.4950)\n",
      "tensor(1.5016)\n",
      "tensor(1.5084)\n",
      "tensor(1.5151)\n",
      "tensor(1.5219)\n",
      "tensor(1.5286)\n",
      "tensor(1.5353)\n",
      "tensor(1.5421)\n",
      "tensor(1.5489)\n",
      "tensor(1.5557)\n",
      "tensor(1.5624)\n",
      "tensor(1.5689)\n",
      "tensor(1.5755)\n",
      "tensor(1.5821)\n",
      "tensor(1.5887)\n",
      "tensor(1.5957)\n",
      "tensor(1.6026)\n",
      "tensor(1.6093)\n",
      "tensor(1.6161)\n",
      "tensor(1.6228)\n",
      "tensor(1.6294)\n",
      "tensor(1.6361)\n",
      "tensor(1.6427)\n",
      "tensor(1.6494)\n",
      "tensor(1.6561)\n",
      "tensor(1.6627)\n",
      "tensor(1.6694)\n",
      "tensor(1.6762)\n",
      "tensor(1.6829)\n",
      "tensor(1.6896)\n",
      "tensor(1.6962)\n",
      "tensor(1.7028)\n",
      "tensor(1.7094)\n",
      "tensor(1.7160)\n",
      "tensor(1.7227)\n",
      "tensor(1.7293)\n",
      "tensor(1.7359)\n",
      "tensor(1.7427)\n",
      "tensor(1.7494)\n",
      "tensor(1.7560)\n",
      "tensor(1.7626)\n",
      "tensor(1.7693)\n",
      "tensor(1.7760)\n",
      "tensor(1.7826)\n",
      "tensor(1.7893)\n",
      "tensor(1.7959)\n",
      "tensor(1.8025)\n",
      "tensor(1.8093)\n",
      "tensor(1.8160)\n",
      "tensor(1.8228)\n",
      "tensor(1.8295)\n",
      "tensor(1.8360)\n",
      "tensor(1.8424)\n",
      "tensor(1.8491)\n",
      "tensor(1.8558)\n",
      "tensor(1.8623)\n",
      "tensor(1.8690)\n",
      "tensor(1.8756)\n",
      "tensor(1.8823)\n",
      "tensor(1.8890)\n",
      "tensor(1.8956)\n",
      "tensor(1.9023)\n",
      "tensor(1.9090)\n",
      "tensor(1.9156)\n",
      "tensor(1.9221)\n",
      "tensor(1.9289)\n",
      "tensor(1.9356)\n",
      "tensor(1.9425)\n",
      "tensor(1.9492)\n",
      "tensor(1.9560)\n",
      "tensor(1.9627)\n",
      "tensor(1.9694)\n",
      "tensor(1.9761)\n",
      "tensor(1.9830)\n",
      "tensor(1.9896)\n",
      "tensor(1.9964)\n",
      "tensor(2.0029)\n",
      "tensor(2.0095)\n",
      "tensor(2.0161)\n",
      "tensor(2.0227)\n",
      "tensor(2.0292)\n",
      "tensor(2.0359)\n",
      "tensor(2.0426)\n",
      "tensor(2.0493)\n",
      "tensor(2.0558)\n",
      "tensor(2.0626)\n",
      "tensor(2.0694)\n",
      "tensor(2.0761)\n",
      "tensor(2.0827)\n",
      "tensor(2.0894)\n",
      "tensor(2.0961)\n",
      "tensor(2.1028)\n",
      "tensor(2.1095)\n",
      "tensor(2.1162)\n",
      "tensor(2.1229)\n",
      "tensor(2.1294)\n",
      "tensor(2.1363)\n",
      "tensor(2.1429)\n",
      "tensor(2.1496)\n",
      "tensor(2.1564)\n",
      "tensor(2.1630)\n",
      "tensor(2.1697)\n",
      "tensor(2.1763)\n",
      "tensor(2.1831)\n",
      "tensor(2.1899)\n",
      "tensor(2.1965)\n",
      "tensor(2.2035)\n",
      "tensor(2.2101)\n",
      "tensor(2.2170)\n",
      "tensor(2.2239)\n",
      "tensor(2.2309)\n",
      "tensor(2.2375)\n",
      "tensor(2.2442)\n",
      "tensor(2.2509)\n",
      "tensor(2.2576)\n",
      "tensor(2.2643)\n",
      "tensor(2.2710)\n",
      "tensor(2.2777)\n",
      "tensor(2.2844)\n",
      "tensor(2.2910)\n",
      "tensor(2.2976)\n",
      "tensor(2.3043)\n",
      "tensor(2.3108)\n",
      "tensor(2.3176)\n",
      "tensor(2.3242)\n",
      "tensor(2.3309)\n",
      "tensor(2.3376)\n",
      "tensor(2.3443)\n",
      "tensor(2.3513)\n",
      "tensor(2.3580)\n",
      "tensor(2.3646)\n",
      "tensor(2.3713)\n",
      "tensor(2.3779)\n",
      "tensor(2.3845)\n",
      "tensor(2.3912)\n",
      "tensor(2.3979)\n",
      "tensor(2.4046)\n",
      "tensor(2.4113)\n",
      "tensor(2.4180)\n",
      "tensor(2.4248)\n",
      "tensor(2.4314)\n",
      "tensor(2.4381)\n",
      "tensor(2.4448)\n",
      "tensor(2.4514)\n",
      "tensor(2.4581)\n",
      "tensor(2.4648)\n",
      "tensor(2.4716)\n",
      "tensor(2.4784)\n",
      "tensor(2.4850)\n",
      "tensor(2.4918)\n",
      "tensor(2.4985)\n",
      "tensor(2.5051)\n",
      "tensor(2.5118)\n",
      "tensor(2.5185)\n",
      "tensor(2.5251)\n",
      "tensor(2.5317)\n",
      "tensor(2.5384)\n",
      "tensor(2.5451)\n",
      "tensor(2.5517)\n",
      "tensor(2.5583)\n",
      "tensor(2.5650)\n",
      "tensor(2.5718)\n",
      "tensor(2.5784)\n",
      "tensor(2.5851)\n",
      "tensor(2.5919)\n",
      "tensor(2.5986)\n",
      "tensor(2.6053)\n",
      "tensor(2.6120)\n",
      "tensor(2.6189)\n",
      "tensor(2.6255)\n",
      "tensor(2.6323)\n",
      "tensor(2.6389)\n",
      "tensor(2.6456)\n",
      "tensor(2.6523)\n",
      "tensor(2.6590)\n",
      "tensor(2.6657)\n",
      "tensor(2.6724)\n",
      "tensor(2.6791)\n",
      "tensor(2.6857)\n",
      "tensor(2.6924)\n",
      "tensor(2.6990)\n",
      "tensor(2.7056)\n",
      "tensor(2.7122)\n",
      "tensor(2.7187)\n",
      "tensor(2.7253)\n",
      "tensor(2.7320)\n",
      "tensor(2.7387)\n",
      "tensor(2.7453)\n",
      "tensor(2.7518)\n",
      "tensor(2.7583)\n",
      "tensor(2.7650)\n",
      "tensor(2.7716)\n",
      "tensor(2.7783)\n",
      "tensor(2.7851)\n",
      "tensor(2.7918)\n",
      "tensor(2.7985)\n",
      "tensor(2.8051)\n",
      "tensor(2.8118)\n",
      "tensor(2.8185)\n",
      "tensor(2.8251)\n",
      "tensor(2.8318)\n",
      "tensor(2.8384)\n",
      "tensor(2.8451)\n",
      "tensor(2.8518)\n",
      "tensor(2.8586)\n",
      "tensor(2.8652)\n",
      "tensor(2.8720)\n",
      "tensor(2.8786)\n",
      "tensor(2.8852)\n",
      "tensor(2.8920)\n",
      "tensor(2.8985)\n",
      "tensor(2.9053)\n",
      "tensor(2.9118)\n",
      "tensor(2.9186)\n",
      "tensor(2.9253)\n",
      "tensor(2.9320)\n",
      "tensor(2.9388)\n",
      "tensor(2.9455)\n",
      "tensor(2.9524)\n",
      "tensor(2.9590)\n",
      "tensor(2.9656)\n",
      "tensor(2.9722)\n",
      "tensor(2.9789)\n",
      "tensor(2.9856)\n",
      "tensor(2.9923)\n",
      "1\n",
      "tensor(0.0066)\n",
      "tensor(0.0133)\n",
      "tensor(0.0199)\n",
      "tensor(0.0266)\n",
      "tensor(0.0332)\n",
      "tensor(0.0399)\n",
      "tensor(0.0465)\n",
      "tensor(0.0533)\n",
      "tensor(0.0600)\n",
      "tensor(0.0667)\n",
      "tensor(0.0734)\n",
      "tensor(0.0801)\n",
      "tensor(0.0867)\n",
      "tensor(0.0934)\n",
      "tensor(0.1000)\n",
      "tensor(0.1066)\n",
      "tensor(0.1133)\n",
      "tensor(0.1199)\n",
      "tensor(0.1266)\n",
      "tensor(0.1333)\n",
      "tensor(0.1399)\n",
      "tensor(0.1466)\n",
      "tensor(0.1532)\n",
      "tensor(0.1597)\n",
      "tensor(0.1664)\n",
      "tensor(0.1731)\n",
      "tensor(0.1798)\n",
      "tensor(0.1863)\n",
      "tensor(0.1929)\n",
      "tensor(0.1995)\n",
      "tensor(0.2062)\n",
      "tensor(0.2128)\n",
      "tensor(0.2194)\n",
      "tensor(0.2261)\n",
      "tensor(0.2327)\n",
      "tensor(0.2395)\n",
      "tensor(0.2461)\n",
      "tensor(0.2528)\n",
      "tensor(0.2595)\n",
      "tensor(0.2663)\n",
      "tensor(0.2728)\n",
      "tensor(0.2795)\n",
      "tensor(0.2862)\n",
      "tensor(0.2929)\n",
      "tensor(0.2997)\n",
      "tensor(0.3065)\n",
      "tensor(0.3133)\n",
      "tensor(0.3199)\n",
      "tensor(0.3266)\n",
      "tensor(0.3333)\n",
      "tensor(0.3400)\n",
      "tensor(0.3467)\n",
      "tensor(0.3535)\n",
      "tensor(0.3602)\n",
      "tensor(0.3669)\n",
      "tensor(0.3736)\n",
      "tensor(0.3803)\n",
      "tensor(0.3872)\n",
      "tensor(0.3940)\n",
      "tensor(0.4009)\n",
      "tensor(0.4076)\n",
      "tensor(0.4144)\n",
      "tensor(0.4211)\n",
      "tensor(0.4278)\n",
      "tensor(0.4344)\n",
      "tensor(0.4412)\n",
      "tensor(0.4480)\n",
      "tensor(0.4547)\n",
      "tensor(0.4614)\n",
      "tensor(0.4681)\n",
      "tensor(0.4748)\n",
      "tensor(0.4815)\n",
      "tensor(0.4884)\n",
      "tensor(0.4952)\n",
      "tensor(0.5019)\n",
      "tensor(0.5087)\n",
      "tensor(0.5156)\n",
      "tensor(0.5223)\n",
      "tensor(0.5291)\n",
      "tensor(0.5361)\n",
      "tensor(0.5430)\n",
      "tensor(0.5498)\n",
      "tensor(0.5566)\n",
      "tensor(0.5634)\n",
      "tensor(0.5703)\n",
      "tensor(0.5771)\n",
      "tensor(0.5836)\n",
      "tensor(0.5903)\n",
      "tensor(0.5969)\n",
      "tensor(0.6036)\n",
      "tensor(0.6102)\n",
      "tensor(0.6169)\n",
      "tensor(0.6236)\n",
      "tensor(0.6303)\n",
      "tensor(0.6370)\n",
      "tensor(0.6437)\n",
      "tensor(0.6504)\n",
      "tensor(0.6570)\n",
      "tensor(0.6638)\n",
      "tensor(0.6704)\n",
      "tensor(0.6770)\n",
      "tensor(0.6837)\n",
      "tensor(0.6904)\n",
      "tensor(0.6971)\n",
      "tensor(0.7038)\n",
      "tensor(0.7104)\n",
      "tensor(0.7170)\n",
      "tensor(0.7237)\n",
      "tensor(0.7304)\n",
      "tensor(0.7371)\n",
      "tensor(0.7438)\n",
      "tensor(0.7505)\n",
      "tensor(0.7572)\n",
      "tensor(0.7639)\n",
      "tensor(0.7705)\n",
      "tensor(0.7771)\n",
      "tensor(0.7838)\n",
      "tensor(0.7904)\n",
      "tensor(0.7970)\n",
      "tensor(0.8034)\n",
      "tensor(0.8101)\n",
      "tensor(0.8167)\n",
      "tensor(0.8234)\n",
      "tensor(0.8300)\n",
      "tensor(0.8368)\n",
      "tensor(0.8435)\n",
      "tensor(0.8501)\n",
      "tensor(0.8568)\n",
      "tensor(0.8634)\n",
      "tensor(0.8701)\n",
      "tensor(0.8768)\n",
      "tensor(0.8836)\n",
      "tensor(0.8903)\n",
      "tensor(0.8969)\n",
      "tensor(0.9037)\n",
      "tensor(0.9105)\n",
      "tensor(0.9171)\n",
      "tensor(0.9240)\n",
      "tensor(0.9308)\n",
      "tensor(0.9376)\n",
      "tensor(0.9444)\n",
      "tensor(0.9511)\n",
      "tensor(0.9577)\n",
      "tensor(0.9644)\n",
      "tensor(0.9710)\n",
      "tensor(0.9776)\n",
      "tensor(0.9841)\n",
      "tensor(0.9910)\n",
      "tensor(0.9979)\n",
      "tensor(1.0046)\n",
      "tensor(1.0113)\n",
      "tensor(1.0180)\n",
      "tensor(1.0246)\n",
      "tensor(1.0313)\n",
      "tensor(1.0379)\n",
      "tensor(1.0446)\n",
      "tensor(1.0513)\n",
      "tensor(1.0579)\n",
      "tensor(1.0647)\n",
      "tensor(1.0715)\n",
      "tensor(1.0782)\n",
      "tensor(1.0850)\n",
      "tensor(1.0917)\n",
      "tensor(1.0985)\n",
      "tensor(1.1052)\n",
      "tensor(1.1121)\n",
      "tensor(1.1189)\n",
      "tensor(1.1256)\n",
      "tensor(1.1322)\n",
      "tensor(1.1390)\n",
      "tensor(1.1457)\n",
      "tensor(1.1524)\n",
      "tensor(1.1592)\n",
      "tensor(1.1659)\n",
      "tensor(1.1726)\n",
      "tensor(1.1793)\n",
      "tensor(1.1860)\n",
      "tensor(1.1925)\n",
      "tensor(1.1993)\n",
      "tensor(1.2061)\n",
      "tensor(1.2127)\n",
      "tensor(1.2194)\n",
      "tensor(1.2261)\n",
      "tensor(1.2328)\n",
      "tensor(1.2395)\n",
      "tensor(1.2462)\n",
      "tensor(1.2528)\n",
      "tensor(1.2595)\n",
      "tensor(1.2661)\n",
      "tensor(1.2727)\n",
      "tensor(1.2792)\n",
      "tensor(1.2856)\n",
      "tensor(1.2924)\n",
      "tensor(1.2990)\n",
      "tensor(1.3059)\n",
      "tensor(1.3127)\n",
      "tensor(1.3193)\n",
      "tensor(1.3260)\n",
      "tensor(1.3329)\n",
      "tensor(1.3397)\n",
      "tensor(1.3463)\n",
      "tensor(1.3530)\n",
      "tensor(1.3597)\n",
      "tensor(1.3664)\n",
      "tensor(1.3731)\n",
      "tensor(1.3798)\n",
      "tensor(1.3866)\n",
      "tensor(1.3931)\n",
      "tensor(1.3999)\n",
      "tensor(1.4066)\n",
      "tensor(1.4133)\n",
      "tensor(1.4202)\n",
      "tensor(1.4268)\n",
      "tensor(1.4335)\n",
      "tensor(1.4401)\n",
      "tensor(1.4467)\n",
      "tensor(1.4534)\n",
      "tensor(1.4601)\n",
      "tensor(1.4666)\n",
      "tensor(1.4733)\n",
      "tensor(1.4799)\n",
      "tensor(1.4865)\n",
      "tensor(1.4933)\n",
      "tensor(1.4999)\n",
      "tensor(1.5066)\n",
      "tensor(1.5133)\n",
      "tensor(1.5200)\n",
      "tensor(1.5267)\n",
      "tensor(1.5334)\n",
      "tensor(1.5402)\n",
      "tensor(1.5470)\n",
      "tensor(1.5538)\n",
      "tensor(1.5605)\n",
      "tensor(1.5669)\n",
      "tensor(1.5736)\n",
      "tensor(1.5802)\n",
      "tensor(1.5868)\n",
      "tensor(1.5937)\n",
      "tensor(1.6006)\n",
      "tensor(1.6073)\n",
      "tensor(1.6140)\n",
      "tensor(1.6207)\n",
      "tensor(1.6274)\n",
      "tensor(1.6340)\n",
      "tensor(1.6406)\n",
      "tensor(1.6473)\n",
      "tensor(1.6540)\n",
      "tensor(1.6606)\n",
      "tensor(1.6673)\n",
      "tensor(1.6740)\n",
      "tensor(1.6807)\n",
      "tensor(1.6874)\n",
      "tensor(1.6940)\n",
      "tensor(1.7006)\n",
      "tensor(1.7072)\n",
      "tensor(1.7138)\n",
      "tensor(1.7204)\n",
      "tensor(1.7270)\n",
      "tensor(1.7337)\n",
      "tensor(1.7404)\n",
      "tensor(1.7470)\n",
      "tensor(1.7536)\n",
      "tensor(1.7602)\n",
      "tensor(1.7668)\n",
      "tensor(1.7735)\n",
      "tensor(1.7802)\n",
      "tensor(1.7868)\n",
      "tensor(1.7934)\n",
      "tensor(1.8000)\n",
      "tensor(1.8067)\n",
      "tensor(1.8135)\n",
      "tensor(1.8202)\n",
      "tensor(1.8268)\n",
      "tensor(1.8333)\n",
      "tensor(1.8398)\n",
      "tensor(1.8465)\n",
      "tensor(1.8531)\n",
      "tensor(1.8597)\n",
      "tensor(1.8663)\n",
      "tensor(1.8729)\n",
      "tensor(1.8797)\n",
      "tensor(1.8863)\n",
      "tensor(1.8929)\n",
      "tensor(1.8996)\n",
      "tensor(1.9063)\n",
      "tensor(1.9128)\n",
      "tensor(1.9193)\n",
      "tensor(1.9260)\n",
      "tensor(1.9328)\n",
      "tensor(1.9396)\n",
      "tensor(1.9464)\n",
      "tensor(1.9531)\n",
      "tensor(1.9598)\n",
      "tensor(1.9665)\n",
      "tensor(1.9732)\n",
      "tensor(1.9800)\n",
      "tensor(1.9867)\n",
      "tensor(1.9934)\n",
      "tensor(1.9999)\n",
      "tensor(2.0066)\n",
      "tensor(2.0132)\n",
      "tensor(2.0197)\n",
      "tensor(2.0263)\n",
      "tensor(2.0330)\n",
      "tensor(2.0396)\n",
      "tensor(2.0463)\n",
      "tensor(2.0528)\n",
      "tensor(2.0596)\n",
      "tensor(2.0664)\n",
      "tensor(2.0731)\n",
      "tensor(2.0796)\n",
      "tensor(2.0864)\n",
      "tensor(2.0930)\n",
      "tensor(2.0997)\n",
      "tensor(2.1065)\n",
      "tensor(2.1131)\n",
      "tensor(2.1197)\n",
      "tensor(2.1263)\n",
      "tensor(2.1331)\n",
      "tensor(2.1397)\n",
      "tensor(2.1464)\n",
      "tensor(2.1531)\n",
      "tensor(2.1598)\n",
      "tensor(2.1664)\n",
      "tensor(2.1730)\n",
      "tensor(2.1798)\n",
      "tensor(2.1866)\n",
      "tensor(2.1932)\n",
      "tensor(2.2002)\n",
      "tensor(2.2069)\n",
      "tensor(2.2137)\n",
      "tensor(2.2206)\n",
      "tensor(2.2276)\n",
      "tensor(2.2342)\n",
      "tensor(2.2409)\n",
      "tensor(2.2476)\n",
      "tensor(2.2543)\n",
      "tensor(2.2609)\n",
      "tensor(2.2677)\n",
      "tensor(2.2744)\n",
      "tensor(2.2810)\n",
      "tensor(2.2876)\n",
      "tensor(2.2942)\n",
      "tensor(2.3009)\n",
      "tensor(2.3075)\n",
      "tensor(2.3142)\n",
      "tensor(2.3208)\n",
      "tensor(2.3275)\n",
      "tensor(2.3342)\n",
      "tensor(2.3409)\n",
      "tensor(2.3478)\n",
      "tensor(2.3545)\n",
      "tensor(2.3611)\n",
      "tensor(2.3679)\n",
      "tensor(2.3745)\n",
      "tensor(2.3810)\n",
      "tensor(2.3877)\n",
      "tensor(2.3944)\n",
      "tensor(2.4012)\n",
      "tensor(2.4078)\n",
      "tensor(2.4145)\n",
      "tensor(2.4212)\n",
      "tensor(2.4279)\n",
      "tensor(2.4346)\n",
      "tensor(2.4412)\n",
      "tensor(2.4478)\n",
      "tensor(2.4545)\n",
      "tensor(2.4612)\n",
      "tensor(2.4680)\n",
      "tensor(2.4747)\n",
      "tensor(2.4813)\n",
      "tensor(2.4881)\n",
      "tensor(2.4948)\n",
      "tensor(2.5014)\n",
      "tensor(2.5080)\n",
      "tensor(2.5148)\n",
      "tensor(2.5214)\n",
      "tensor(2.5280)\n",
      "tensor(2.5347)\n",
      "tensor(2.5413)\n",
      "tensor(2.5479)\n",
      "tensor(2.5546)\n",
      "tensor(2.5613)\n",
      "tensor(2.5680)\n",
      "tensor(2.5746)\n",
      "tensor(2.5813)\n",
      "tensor(2.5881)\n",
      "tensor(2.5948)\n",
      "tensor(2.6014)\n",
      "tensor(2.6081)\n",
      "tensor(2.6150)\n",
      "tensor(2.6216)\n",
      "tensor(2.6284)\n",
      "tensor(2.6350)\n",
      "tensor(2.6417)\n",
      "tensor(2.6484)\n",
      "tensor(2.6551)\n",
      "tensor(2.6618)\n",
      "tensor(2.6685)\n",
      "tensor(2.6751)\n",
      "tensor(2.6817)\n",
      "tensor(2.6884)\n",
      "tensor(2.6951)\n",
      "tensor(2.7016)\n",
      "tensor(2.7082)\n",
      "tensor(2.7147)\n",
      "tensor(2.7214)\n",
      "tensor(2.7281)\n",
      "tensor(2.7347)\n",
      "tensor(2.7413)\n",
      "tensor(2.7477)\n",
      "tensor(2.7543)\n",
      "tensor(2.7609)\n",
      "tensor(2.7675)\n",
      "tensor(2.7743)\n",
      "tensor(2.7810)\n",
      "tensor(2.7877)\n",
      "tensor(2.7944)\n",
      "tensor(2.8010)\n",
      "tensor(2.8077)\n",
      "tensor(2.8144)\n",
      "tensor(2.8210)\n",
      "tensor(2.8276)\n",
      "tensor(2.8342)\n",
      "tensor(2.8409)\n",
      "tensor(2.8475)\n",
      "tensor(2.8543)\n",
      "tensor(2.8610)\n",
      "tensor(2.8677)\n",
      "tensor(2.8742)\n",
      "tensor(2.8808)\n",
      "tensor(2.8876)\n",
      "tensor(2.8942)\n",
      "tensor(2.9009)\n",
      "tensor(2.9075)\n",
      "tensor(2.9142)\n",
      "tensor(2.9209)\n",
      "tensor(2.9276)\n",
      "tensor(2.9344)\n",
      "tensor(2.9411)\n",
      "tensor(2.9480)\n",
      "tensor(2.9546)\n",
      "tensor(2.9612)\n",
      "tensor(2.9677)\n",
      "tensor(2.9744)\n",
      "tensor(2.9812)\n",
      "tensor(2.9877)\n",
      "2\n",
      "tensor(0.0066)\n",
      "tensor(0.0133)\n",
      "tensor(0.0199)\n",
      "tensor(0.0266)\n",
      "tensor(0.0332)\n",
      "tensor(0.0398)\n",
      "tensor(0.0465)\n",
      "tensor(0.0532)\n",
      "tensor(0.0599)\n",
      "tensor(0.0666)\n",
      "tensor(0.0734)\n",
      "tensor(0.0800)\n",
      "tensor(0.0866)\n",
      "tensor(0.0933)\n",
      "tensor(0.0998)\n",
      "tensor(0.1065)\n",
      "tensor(0.1131)\n",
      "tensor(0.1198)\n",
      "tensor(0.1264)\n",
      "tensor(0.1330)\n",
      "tensor(0.1396)\n",
      "tensor(0.1463)\n",
      "tensor(0.1530)\n",
      "tensor(0.1595)\n",
      "tensor(0.1661)\n",
      "tensor(0.1729)\n",
      "tensor(0.1796)\n",
      "tensor(0.1861)\n",
      "tensor(0.1927)\n",
      "tensor(0.1993)\n",
      "tensor(0.2059)\n",
      "tensor(0.2125)\n",
      "tensor(0.2191)\n",
      "tensor(0.2258)\n",
      "tensor(0.2323)\n",
      "tensor(0.2391)\n",
      "tensor(0.2457)\n",
      "tensor(0.2524)\n",
      "tensor(0.2591)\n",
      "tensor(0.2658)\n",
      "tensor(0.2724)\n",
      "tensor(0.2790)\n",
      "tensor(0.2857)\n",
      "tensor(0.2923)\n",
      "tensor(0.2990)\n",
      "tensor(0.3058)\n",
      "tensor(0.3126)\n",
      "tensor(0.3192)\n",
      "tensor(0.3259)\n",
      "tensor(0.3325)\n",
      "tensor(0.3392)\n",
      "tensor(0.3460)\n",
      "tensor(0.3527)\n",
      "tensor(0.3594)\n",
      "tensor(0.3661)\n",
      "tensor(0.3727)\n",
      "tensor(0.3794)\n",
      "tensor(0.3862)\n",
      "tensor(0.3931)\n",
      "tensor(0.3999)\n",
      "tensor(0.4066)\n",
      "tensor(0.4134)\n",
      "tensor(0.4201)\n",
      "tensor(0.4267)\n",
      "tensor(0.4333)\n",
      "tensor(0.4402)\n",
      "tensor(0.4469)\n",
      "tensor(0.4536)\n",
      "tensor(0.4603)\n",
      "tensor(0.4670)\n",
      "tensor(0.4737)\n",
      "tensor(0.4804)\n",
      "tensor(0.4872)\n",
      "tensor(0.4940)\n",
      "tensor(0.5007)\n",
      "tensor(0.5075)\n",
      "tensor(0.5143)\n",
      "tensor(0.5210)\n",
      "tensor(0.5278)\n",
      "tensor(0.5347)\n",
      "tensor(0.5416)\n",
      "tensor(0.5484)\n",
      "tensor(0.5551)\n",
      "tensor(0.5619)\n",
      "tensor(0.5688)\n",
      "tensor(0.5755)\n",
      "tensor(0.5820)\n",
      "tensor(0.5887)\n",
      "tensor(0.5953)\n",
      "tensor(0.6020)\n",
      "tensor(0.6086)\n",
      "tensor(0.6152)\n",
      "tensor(0.6219)\n",
      "tensor(0.6285)\n",
      "tensor(0.6352)\n",
      "tensor(0.6419)\n",
      "tensor(0.6486)\n",
      "tensor(0.6552)\n",
      "tensor(0.6620)\n",
      "tensor(0.6686)\n",
      "tensor(0.6752)\n",
      "tensor(0.6819)\n",
      "tensor(0.6886)\n",
      "tensor(0.6953)\n",
      "tensor(0.7020)\n",
      "tensor(0.7085)\n",
      "tensor(0.7152)\n",
      "tensor(0.7218)\n",
      "tensor(0.7285)\n",
      "tensor(0.7352)\n",
      "tensor(0.7419)\n",
      "tensor(0.7486)\n",
      "tensor(0.7552)\n",
      "tensor(0.7619)\n",
      "tensor(0.7685)\n",
      "tensor(0.7751)\n",
      "tensor(0.7817)\n",
      "tensor(0.7884)\n",
      "tensor(0.7949)\n",
      "tensor(0.8013)\n",
      "tensor(0.8080)\n",
      "tensor(0.8145)\n",
      "tensor(0.8212)\n",
      "tensor(0.8279)\n",
      "tensor(0.8347)\n",
      "tensor(0.8413)\n",
      "tensor(0.8479)\n",
      "tensor(0.8546)\n",
      "tensor(0.8612)\n",
      "tensor(0.8678)\n",
      "tensor(0.8745)\n",
      "tensor(0.8813)\n",
      "tensor(0.8880)\n",
      "tensor(0.8946)\n",
      "tensor(0.9014)\n",
      "tensor(0.9081)\n",
      "tensor(0.9148)\n",
      "tensor(0.9216)\n",
      "tensor(0.9284)\n",
      "tensor(0.9351)\n",
      "tensor(0.9419)\n",
      "tensor(0.9486)\n",
      "tensor(0.9553)\n",
      "tensor(0.9619)\n",
      "tensor(0.9685)\n",
      "tensor(0.9750)\n",
      "tensor(0.9816)\n",
      "tensor(0.9884)\n",
      "tensor(0.9953)\n",
      "tensor(1.0020)\n",
      "tensor(1.0086)\n",
      "tensor(1.0153)\n",
      "tensor(1.0220)\n",
      "tensor(1.0286)\n",
      "tensor(1.0352)\n",
      "tensor(1.0419)\n",
      "tensor(1.0485)\n",
      "tensor(1.0552)\n",
      "tensor(1.0620)\n",
      "tensor(1.0688)\n",
      "tensor(1.0754)\n",
      "tensor(1.0822)\n",
      "tensor(1.0889)\n",
      "tensor(1.0957)\n",
      "tensor(1.1025)\n",
      "tensor(1.1093)\n",
      "tensor(1.1161)\n",
      "tensor(1.1228)\n",
      "tensor(1.1294)\n",
      "tensor(1.1362)\n",
      "tensor(1.1429)\n",
      "tensor(1.1496)\n",
      "tensor(1.1564)\n",
      "tensor(1.1630)\n",
      "tensor(1.1697)\n",
      "tensor(1.1764)\n",
      "tensor(1.1830)\n",
      "tensor(1.1895)\n",
      "tensor(1.1963)\n",
      "tensor(1.2030)\n",
      "tensor(1.2097)\n",
      "tensor(1.2163)\n",
      "tensor(1.2230)\n",
      "tensor(1.2297)\n",
      "tensor(1.2364)\n",
      "tensor(1.2431)\n",
      "tensor(1.2497)\n",
      "tensor(1.2563)\n",
      "tensor(1.2629)\n",
      "tensor(1.2695)\n",
      "tensor(1.2760)\n",
      "tensor(1.2825)\n",
      "tensor(1.2891)\n",
      "tensor(1.2957)\n",
      "tensor(1.3026)\n",
      "tensor(1.3094)\n",
      "tensor(1.3160)\n",
      "tensor(1.3226)\n",
      "tensor(1.3295)\n",
      "tensor(1.3362)\n",
      "tensor(1.3429)\n",
      "tensor(1.3495)\n",
      "tensor(1.3561)\n",
      "tensor(1.3628)\n",
      "tensor(1.3695)\n",
      "tensor(1.3762)\n",
      "tensor(1.3829)\n",
      "tensor(1.3895)\n",
      "tensor(1.3962)\n",
      "tensor(1.4029)\n",
      "tensor(1.4096)\n",
      "tensor(1.4163)\n",
      "tensor(1.4229)\n",
      "tensor(1.4295)\n",
      "tensor(1.4361)\n",
      "tensor(1.4427)\n",
      "tensor(1.4494)\n",
      "tensor(1.4561)\n",
      "tensor(1.4626)\n",
      "tensor(1.4693)\n",
      "tensor(1.4759)\n",
      "tensor(1.4825)\n",
      "tensor(1.4892)\n",
      "tensor(1.4958)\n",
      "tensor(1.5025)\n",
      "tensor(1.5092)\n",
      "tensor(1.5159)\n",
      "tensor(1.5226)\n",
      "tensor(1.5292)\n",
      "tensor(1.5360)\n",
      "tensor(1.5428)\n",
      "tensor(1.5495)\n",
      "tensor(1.5562)\n",
      "tensor(1.5626)\n",
      "tensor(1.5693)\n",
      "tensor(1.5758)\n",
      "tensor(1.5825)\n",
      "tensor(1.5893)\n",
      "tensor(1.5961)\n",
      "tensor(1.6027)\n",
      "tensor(1.6095)\n",
      "tensor(1.6162)\n",
      "tensor(1.6228)\n",
      "tensor(1.6293)\n",
      "tensor(1.6360)\n",
      "tensor(1.6426)\n",
      "tensor(1.6493)\n",
      "tensor(1.6559)\n",
      "tensor(1.6627)\n",
      "tensor(1.6694)\n",
      "tensor(1.6761)\n",
      "tensor(1.6827)\n",
      "tensor(1.6893)\n",
      "tensor(1.6960)\n",
      "tensor(1.7025)\n",
      "tensor(1.7091)\n",
      "tensor(1.7157)\n",
      "tensor(1.7223)\n",
      "tensor(1.7290)\n",
      "tensor(1.7356)\n",
      "tensor(1.7423)\n",
      "tensor(1.7489)\n",
      "tensor(1.7555)\n",
      "tensor(1.7620)\n",
      "tensor(1.7686)\n",
      "tensor(1.7753)\n",
      "tensor(1.7818)\n",
      "tensor(1.7884)\n",
      "tensor(1.7950)\n",
      "tensor(1.8018)\n",
      "tensor(1.8085)\n",
      "tensor(1.8151)\n",
      "tensor(1.8218)\n",
      "tensor(1.8283)\n",
      "tensor(1.8347)\n",
      "tensor(1.8413)\n",
      "tensor(1.8480)\n",
      "tensor(1.8545)\n",
      "tensor(1.8611)\n",
      "tensor(1.8677)\n",
      "tensor(1.8745)\n",
      "tensor(1.8810)\n",
      "tensor(1.8876)\n",
      "tensor(1.8943)\n",
      "tensor(1.9010)\n",
      "tensor(1.9075)\n",
      "tensor(1.9141)\n",
      "tensor(1.9207)\n",
      "tensor(1.9274)\n",
      "tensor(1.9342)\n",
      "tensor(1.9410)\n",
      "tensor(1.9477)\n",
      "tensor(1.9544)\n",
      "tensor(1.9610)\n",
      "tensor(1.9677)\n",
      "tensor(1.9746)\n",
      "tensor(1.9812)\n",
      "tensor(1.9879)\n",
      "tensor(1.9944)\n",
      "tensor(2.0010)\n",
      "tensor(2.0076)\n",
      "tensor(2.0142)\n",
      "tensor(2.0207)\n",
      "tensor(2.0274)\n",
      "tensor(2.0340)\n",
      "tensor(2.0406)\n",
      "tensor(2.0471)\n",
      "tensor(2.0538)\n",
      "tensor(2.0605)\n",
      "tensor(2.0672)\n",
      "tensor(2.0738)\n",
      "tensor(2.0805)\n",
      "tensor(2.0872)\n",
      "tensor(2.0939)\n",
      "tensor(2.1006)\n",
      "tensor(2.1072)\n",
      "tensor(2.1138)\n",
      "tensor(2.1204)\n",
      "tensor(2.1271)\n",
      "tensor(2.1338)\n",
      "tensor(2.1404)\n",
      "tensor(2.1471)\n",
      "tensor(2.1538)\n",
      "tensor(2.1604)\n",
      "tensor(2.1670)\n",
      "tensor(2.1738)\n",
      "tensor(2.1806)\n",
      "tensor(2.1872)\n",
      "tensor(2.1942)\n",
      "tensor(2.2008)\n",
      "tensor(2.2077)\n",
      "tensor(2.2145)\n",
      "tensor(2.2214)\n",
      "tensor(2.2280)\n",
      "tensor(2.2348)\n",
      "tensor(2.2414)\n",
      "tensor(2.2481)\n",
      "tensor(2.2548)\n",
      "tensor(2.2615)\n",
      "tensor(2.2681)\n",
      "tensor(2.2747)\n",
      "tensor(2.2814)\n",
      "tensor(2.2880)\n",
      "tensor(2.2946)\n",
      "tensor(2.3012)\n",
      "tensor(2.3079)\n",
      "tensor(2.3145)\n",
      "tensor(2.3211)\n",
      "tensor(2.3278)\n",
      "tensor(2.3346)\n",
      "tensor(2.3414)\n",
      "tensor(2.3481)\n",
      "tensor(2.3547)\n",
      "tensor(2.3615)\n",
      "tensor(2.3680)\n",
      "tensor(2.3746)\n",
      "tensor(2.3813)\n",
      "tensor(2.3879)\n",
      "tensor(2.3947)\n",
      "tensor(2.4013)\n",
      "tensor(2.4080)\n",
      "tensor(2.4148)\n",
      "tensor(2.4214)\n",
      "tensor(2.4281)\n",
      "tensor(2.4347)\n",
      "tensor(2.4413)\n",
      "tensor(2.4480)\n",
      "tensor(2.4547)\n",
      "tensor(2.4614)\n",
      "tensor(2.4682)\n",
      "tensor(2.4748)\n",
      "tensor(2.4815)\n",
      "tensor(2.4883)\n",
      "tensor(2.4949)\n",
      "tensor(2.5015)\n",
      "tensor(2.5082)\n",
      "tensor(2.5149)\n",
      "tensor(2.5214)\n",
      "tensor(2.5281)\n",
      "tensor(2.5348)\n",
      "tensor(2.5413)\n",
      "tensor(2.5480)\n",
      "tensor(2.5546)\n",
      "tensor(2.5614)\n",
      "tensor(2.5680)\n",
      "tensor(2.5746)\n",
      "tensor(2.5814)\n",
      "tensor(2.5881)\n",
      "tensor(2.5947)\n",
      "tensor(2.6014)\n",
      "tensor(2.6082)\n",
      "tensor(2.6148)\n",
      "tensor(2.6215)\n",
      "tensor(2.6281)\n",
      "tensor(2.6348)\n",
      "tensor(2.6415)\n",
      "tensor(2.6481)\n",
      "tensor(2.6548)\n",
      "tensor(2.6615)\n",
      "tensor(2.6682)\n",
      "tensor(2.6748)\n",
      "tensor(2.6814)\n",
      "tensor(2.6881)\n",
      "tensor(2.6946)\n",
      "tensor(2.7011)\n",
      "tensor(2.7076)\n",
      "tensor(2.7143)\n",
      "tensor(2.7210)\n",
      "tensor(2.7276)\n",
      "tensor(2.7342)\n",
      "tensor(2.7406)\n",
      "tensor(2.7471)\n",
      "tensor(2.7538)\n",
      "tensor(2.7604)\n",
      "tensor(2.7670)\n",
      "tensor(2.7737)\n",
      "tensor(2.7804)\n",
      "tensor(2.7871)\n",
      "tensor(2.7937)\n",
      "tensor(2.8004)\n",
      "tensor(2.8071)\n",
      "tensor(2.8137)\n",
      "tensor(2.8203)\n",
      "tensor(2.8269)\n",
      "tensor(2.8335)\n",
      "tensor(2.8402)\n",
      "tensor(2.8469)\n",
      "tensor(2.8536)\n",
      "tensor(2.8603)\n",
      "tensor(2.8668)\n",
      "tensor(2.8734)\n",
      "tensor(2.8802)\n",
      "tensor(2.8867)\n",
      "tensor(2.8934)\n",
      "tensor(2.9000)\n",
      "tensor(2.9067)\n",
      "tensor(2.9134)\n",
      "tensor(2.9201)\n",
      "tensor(2.9269)\n",
      "tensor(2.9335)\n",
      "tensor(2.9404)\n",
      "tensor(2.9469)\n",
      "tensor(2.9536)\n",
      "tensor(2.9601)\n",
      "tensor(2.9667)\n",
      "tensor(2.9734)\n",
      "tensor(2.9800)\n",
      "3\n",
      "tensor(0.0066)\n",
      "tensor(0.0133)\n",
      "tensor(0.0198)\n",
      "tensor(0.0265)\n",
      "tensor(0.0332)\n",
      "tensor(0.0398)\n",
      "tensor(0.0464)\n",
      "tensor(0.0531)\n",
      "tensor(0.0597)\n",
      "tensor(0.0664)\n",
      "tensor(0.0732)\n",
      "tensor(0.0798)\n",
      "tensor(0.0864)\n",
      "tensor(0.0930)\n",
      "tensor(0.0996)\n",
      "tensor(0.1062)\n",
      "tensor(0.1128)\n",
      "tensor(0.1195)\n",
      "tensor(0.1261)\n",
      "tensor(0.1327)\n",
      "tensor(0.1393)\n",
      "tensor(0.1459)\n",
      "tensor(0.1526)\n",
      "tensor(0.1591)\n",
      "tensor(0.1657)\n",
      "tensor(0.1725)\n",
      "tensor(0.1791)\n",
      "tensor(0.1857)\n",
      "tensor(0.1923)\n",
      "tensor(0.1988)\n",
      "tensor(0.2054)\n",
      "tensor(0.2120)\n",
      "tensor(0.2186)\n",
      "tensor(0.2252)\n",
      "tensor(0.2318)\n",
      "tensor(0.2385)\n",
      "tensor(0.2452)\n",
      "tensor(0.2518)\n",
      "tensor(0.2584)\n",
      "tensor(0.2652)\n",
      "tensor(0.2717)\n",
      "tensor(0.2783)\n",
      "tensor(0.2849)\n",
      "tensor(0.2916)\n",
      "tensor(0.2983)\n",
      "tensor(0.3051)\n",
      "tensor(0.3119)\n",
      "tensor(0.3184)\n",
      "tensor(0.3251)\n",
      "tensor(0.3317)\n",
      "tensor(0.3384)\n",
      "tensor(0.3452)\n",
      "tensor(0.3519)\n",
      "tensor(0.3586)\n",
      "tensor(0.3653)\n",
      "tensor(0.3719)\n",
      "tensor(0.3786)\n",
      "tensor(0.3854)\n",
      "tensor(0.3922)\n",
      "tensor(0.3990)\n",
      "tensor(0.4057)\n",
      "tensor(0.4125)\n",
      "tensor(0.4192)\n",
      "tensor(0.4258)\n",
      "tensor(0.4324)\n",
      "tensor(0.4392)\n",
      "tensor(0.4459)\n",
      "tensor(0.4526)\n",
      "tensor(0.4592)\n",
      "tensor(0.4660)\n",
      "tensor(0.4726)\n",
      "tensor(0.4793)\n",
      "tensor(0.4862)\n",
      "tensor(0.4929)\n",
      "tensor(0.4996)\n",
      "tensor(0.5064)\n",
      "tensor(0.5132)\n",
      "tensor(0.5200)\n",
      "tensor(0.5267)\n",
      "tensor(0.5337)\n",
      "tensor(0.5405)\n",
      "tensor(0.5473)\n",
      "tensor(0.5540)\n",
      "tensor(0.5608)\n",
      "tensor(0.5676)\n",
      "tensor(0.5743)\n",
      "tensor(0.5809)\n",
      "tensor(0.5875)\n",
      "tensor(0.5941)\n",
      "tensor(0.6008)\n",
      "tensor(0.6074)\n",
      "tensor(0.6140)\n",
      "tensor(0.6207)\n",
      "tensor(0.6273)\n",
      "tensor(0.6339)\n",
      "tensor(0.6406)\n",
      "tensor(0.6473)\n",
      "tensor(0.6540)\n",
      "tensor(0.6607)\n",
      "tensor(0.6673)\n",
      "tensor(0.6740)\n",
      "tensor(0.6807)\n",
      "tensor(0.6873)\n",
      "tensor(0.6940)\n",
      "tensor(0.7006)\n",
      "tensor(0.7072)\n",
      "tensor(0.7139)\n",
      "tensor(0.7205)\n",
      "tensor(0.7272)\n",
      "tensor(0.7338)\n",
      "tensor(0.7405)\n",
      "tensor(0.7472)\n",
      "tensor(0.7538)\n",
      "tensor(0.7604)\n",
      "tensor(0.7671)\n",
      "tensor(0.7737)\n",
      "tensor(0.7803)\n",
      "tensor(0.7869)\n",
      "tensor(0.7935)\n",
      "tensor(0.7999)\n",
      "tensor(0.8065)\n",
      "tensor(0.8131)\n",
      "tensor(0.8197)\n",
      "tensor(0.8264)\n",
      "tensor(0.8331)\n",
      "tensor(0.8398)\n",
      "tensor(0.8464)\n",
      "tensor(0.8530)\n",
      "tensor(0.8596)\n",
      "tensor(0.8662)\n",
      "tensor(0.8729)\n",
      "tensor(0.8797)\n",
      "tensor(0.8864)\n",
      "tensor(0.8930)\n",
      "tensor(0.8997)\n",
      "tensor(0.9064)\n",
      "tensor(0.9131)\n",
      "tensor(0.9199)\n",
      "tensor(0.9266)\n",
      "tensor(0.9334)\n",
      "tensor(0.9402)\n",
      "tensor(0.9469)\n",
      "tensor(0.9535)\n",
      "tensor(0.9601)\n",
      "tensor(0.9666)\n",
      "tensor(0.9732)\n",
      "tensor(0.9797)\n",
      "tensor(0.9865)\n",
      "tensor(0.9934)\n",
      "tensor(1.0001)\n",
      "tensor(1.0067)\n",
      "tensor(1.0134)\n",
      "tensor(1.0201)\n",
      "tensor(1.0267)\n",
      "tensor(1.0332)\n",
      "tensor(1.0399)\n",
      "tensor(1.0465)\n",
      "tensor(1.0531)\n",
      "tensor(1.0599)\n",
      "tensor(1.0667)\n",
      "tensor(1.0734)\n",
      "tensor(1.0801)\n",
      "tensor(1.0868)\n",
      "tensor(1.0936)\n",
      "tensor(1.1003)\n",
      "tensor(1.1072)\n",
      "tensor(1.1139)\n",
      "tensor(1.1206)\n",
      "tensor(1.1273)\n",
      "tensor(1.1340)\n",
      "tensor(1.1407)\n",
      "tensor(1.1474)\n",
      "tensor(1.1542)\n",
      "tensor(1.1607)\n",
      "tensor(1.1674)\n",
      "tensor(1.1741)\n",
      "tensor(1.1807)\n",
      "tensor(1.1872)\n",
      "tensor(1.1939)\n",
      "tensor(1.2007)\n",
      "tensor(1.2073)\n",
      "tensor(1.2139)\n",
      "tensor(1.2206)\n",
      "tensor(1.2272)\n",
      "tensor(1.2339)\n",
      "tensor(1.2406)\n",
      "tensor(1.2472)\n",
      "tensor(1.2538)\n",
      "tensor(1.2603)\n",
      "tensor(1.2669)\n",
      "tensor(1.2734)\n",
      "tensor(1.2798)\n",
      "tensor(1.2865)\n",
      "tensor(1.2931)\n",
      "tensor(1.2999)\n",
      "tensor(1.3066)\n",
      "tensor(1.3132)\n",
      "tensor(1.3199)\n",
      "tensor(1.3267)\n",
      "tensor(1.3334)\n",
      "tensor(1.3401)\n",
      "tensor(1.3467)\n",
      "tensor(1.3533)\n",
      "tensor(1.3600)\n",
      "tensor(1.3667)\n",
      "tensor(1.3733)\n",
      "tensor(1.3800)\n",
      "tensor(1.3865)\n",
      "tensor(1.3932)\n",
      "tensor(1.3999)\n",
      "tensor(1.4065)\n",
      "tensor(1.4133)\n",
      "tensor(1.4198)\n",
      "tensor(1.4264)\n",
      "tensor(1.4330)\n",
      "tensor(1.4396)\n",
      "tensor(1.4463)\n",
      "tensor(1.4529)\n",
      "tensor(1.4594)\n",
      "tensor(1.4660)\n",
      "tensor(1.4726)\n",
      "tensor(1.4792)\n",
      "tensor(1.4859)\n",
      "tensor(1.4925)\n",
      "tensor(1.4992)\n",
      "tensor(1.5059)\n",
      "tensor(1.5126)\n",
      "tensor(1.5192)\n",
      "tensor(1.5258)\n",
      "tensor(1.5326)\n",
      "tensor(1.5394)\n",
      "tensor(1.5461)\n",
      "tensor(1.5528)\n",
      "tensor(1.5592)\n",
      "tensor(1.5658)\n",
      "tensor(1.5723)\n",
      "tensor(1.5790)\n",
      "tensor(1.5858)\n",
      "tensor(1.5926)\n",
      "tensor(1.5992)\n",
      "tensor(1.6059)\n",
      "tensor(1.6125)\n",
      "tensor(1.6192)\n",
      "tensor(1.6257)\n",
      "tensor(1.6324)\n",
      "tensor(1.6390)\n",
      "tensor(1.6457)\n",
      "tensor(1.6523)\n",
      "tensor(1.6590)\n",
      "tensor(1.6657)\n",
      "tensor(1.6724)\n",
      "tensor(1.6790)\n",
      "tensor(1.6857)\n",
      "tensor(1.6923)\n",
      "tensor(1.6989)\n",
      "tensor(1.7055)\n",
      "tensor(1.7120)\n",
      "tensor(1.7185)\n",
      "tensor(1.7252)\n",
      "tensor(1.7319)\n",
      "tensor(1.7385)\n",
      "tensor(1.7451)\n",
      "tensor(1.7516)\n",
      "tensor(1.7582)\n",
      "tensor(1.7648)\n",
      "tensor(1.7714)\n",
      "tensor(1.7780)\n",
      "tensor(1.7845)\n",
      "tensor(1.7911)\n",
      "tensor(1.7979)\n",
      "tensor(1.8045)\n",
      "tensor(1.8112)\n",
      "tensor(1.8178)\n",
      "tensor(1.8243)\n",
      "tensor(1.8308)\n",
      "tensor(1.8374)\n",
      "tensor(1.8440)\n",
      "tensor(1.8506)\n",
      "tensor(1.8572)\n",
      "tensor(1.8638)\n",
      "tensor(1.8704)\n",
      "tensor(1.8770)\n",
      "tensor(1.8836)\n",
      "tensor(1.8903)\n",
      "tensor(1.8969)\n",
      "tensor(1.9035)\n",
      "tensor(1.9100)\n",
      "tensor(1.9167)\n",
      "tensor(1.9234)\n",
      "tensor(1.9302)\n",
      "tensor(1.9370)\n",
      "tensor(1.9436)\n",
      "tensor(1.9503)\n",
      "tensor(1.9569)\n",
      "tensor(1.9636)\n",
      "tensor(1.9704)\n",
      "tensor(1.9771)\n",
      "tensor(1.9838)\n",
      "tensor(1.9903)\n",
      "tensor(1.9969)\n",
      "tensor(2.0034)\n",
      "tensor(2.0100)\n",
      "tensor(2.0165)\n",
      "tensor(2.0232)\n",
      "tensor(2.0299)\n",
      "tensor(2.0364)\n",
      "tensor(2.0428)\n",
      "tensor(2.0495)\n",
      "tensor(2.0563)\n",
      "tensor(2.0630)\n",
      "tensor(2.0695)\n",
      "tensor(2.0763)\n",
      "tensor(2.0829)\n",
      "tensor(2.0896)\n",
      "tensor(2.0962)\n",
      "tensor(2.1028)\n",
      "tensor(2.1094)\n",
      "tensor(2.1160)\n",
      "tensor(2.1227)\n",
      "tensor(2.1293)\n",
      "tensor(2.1360)\n",
      "tensor(2.1427)\n",
      "tensor(2.1493)\n",
      "tensor(2.1559)\n",
      "tensor(2.1625)\n",
      "tensor(2.1693)\n",
      "tensor(2.1760)\n",
      "tensor(2.1826)\n",
      "tensor(2.1896)\n",
      "tensor(2.1962)\n",
      "tensor(2.2030)\n",
      "tensor(2.2098)\n",
      "tensor(2.2167)\n",
      "tensor(2.2233)\n",
      "tensor(2.2300)\n",
      "tensor(2.2366)\n",
      "tensor(2.2432)\n",
      "tensor(2.2499)\n",
      "tensor(2.2567)\n",
      "tensor(2.2633)\n",
      "tensor(2.2700)\n",
      "tensor(2.2766)\n",
      "tensor(2.2832)\n",
      "tensor(2.2898)\n",
      "tensor(2.2964)\n",
      "tensor(2.3030)\n",
      "tensor(2.3097)\n",
      "tensor(2.3163)\n",
      "tensor(2.3230)\n",
      "tensor(2.3297)\n",
      "tensor(2.3365)\n",
      "tensor(2.3431)\n",
      "tensor(2.3498)\n",
      "tensor(2.3565)\n",
      "tensor(2.3630)\n",
      "tensor(2.3696)\n",
      "tensor(2.3762)\n",
      "tensor(2.3829)\n",
      "tensor(2.3896)\n",
      "tensor(2.3962)\n",
      "tensor(2.4029)\n",
      "tensor(2.4097)\n",
      "tensor(2.4162)\n",
      "tensor(2.4230)\n",
      "tensor(2.4296)\n",
      "tensor(2.4361)\n",
      "tensor(2.4428)\n",
      "tensor(2.4495)\n",
      "tensor(2.4562)\n",
      "tensor(2.4630)\n",
      "tensor(2.4695)\n",
      "tensor(2.4763)\n",
      "tensor(2.4830)\n",
      "tensor(2.4896)\n",
      "tensor(2.4962)\n",
      "tensor(2.5029)\n",
      "tensor(2.5095)\n",
      "tensor(2.5161)\n",
      "tensor(2.5227)\n",
      "tensor(2.5294)\n",
      "tensor(2.5359)\n",
      "tensor(2.5426)\n",
      "tensor(2.5493)\n",
      "tensor(2.5559)\n",
      "tensor(2.5625)\n",
      "tensor(2.5692)\n",
      "tensor(2.5759)\n",
      "tensor(2.5826)\n",
      "tensor(2.5892)\n",
      "tensor(2.5959)\n",
      "tensor(2.6027)\n",
      "tensor(2.6093)\n",
      "tensor(2.6160)\n",
      "tensor(2.6227)\n",
      "tensor(2.6293)\n",
      "tensor(2.6360)\n",
      "tensor(2.6426)\n",
      "tensor(2.6493)\n",
      "tensor(2.6560)\n",
      "tensor(2.6626)\n",
      "tensor(2.6692)\n",
      "tensor(2.6758)\n",
      "tensor(2.6825)\n",
      "tensor(2.6890)\n",
      "tensor(2.6956)\n",
      "tensor(2.7021)\n",
      "tensor(2.7088)\n",
      "tensor(2.7154)\n",
      "tensor(2.7220)\n",
      "tensor(2.7285)\n",
      "tensor(2.7350)\n",
      "tensor(2.7415)\n",
      "tensor(2.7481)\n",
      "tensor(2.7547)\n",
      "tensor(2.7614)\n",
      "tensor(2.7681)\n",
      "tensor(2.7747)\n",
      "tensor(2.7814)\n",
      "tensor(2.7881)\n",
      "tensor(2.7947)\n",
      "tensor(2.8014)\n",
      "tensor(2.8080)\n",
      "tensor(2.8146)\n",
      "tensor(2.8211)\n",
      "tensor(2.8277)\n",
      "tensor(2.8344)\n",
      "tensor(2.8411)\n",
      "tensor(2.8478)\n",
      "tensor(2.8544)\n",
      "tensor(2.8610)\n",
      "tensor(2.8675)\n",
      "tensor(2.8742)\n",
      "tensor(2.8808)\n",
      "tensor(2.8875)\n",
      "tensor(2.8940)\n",
      "tensor(2.9007)\n",
      "tensor(2.9074)\n",
      "tensor(2.9140)\n",
      "tensor(2.9209)\n",
      "tensor(2.9275)\n",
      "tensor(2.9344)\n",
      "tensor(2.9409)\n",
      "tensor(2.9475)\n",
      "tensor(2.9540)\n",
      "tensor(2.9606)\n",
      "tensor(2.9672)\n",
      "tensor(2.9738)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "model = TaggerNet(dataset.get_vocab_size(), 10, dataset.get_tagset_size())\n",
    "dataloader = DataLoader(dataset, batch_size=20, collate_fn=collate_fn)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(5):\n",
    "  loss_sum = torch.tensor(0.)\n",
    "  for sentences, tags, lengths in dataloader:\n",
    "    predicted = model(sentences, lengths)\n",
    "    loss = loss_function(predicted, tags)\n",
    "    loss.backward()\n",
    "\n",
    "    clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      loss_sum += loss\n",
    "      print(loss_sum / len(dataloader))\n",
    "  print(epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- W tym zadaniu zaimplementowaliśmy dynamiczne tworzenie paczek danych, w ramach których uzupełnialiśmy sekwencje do takiej samej długości. Czasami stosowaną techniką jest posortowanie zbioru danych, tak aby sekwencje były ułożone od najdłuższej do najkrótszej, a następnie dynamicznie tworzy się kolejne paczeki danych iterując po tak ułożonym zbiorze. Jakie są wady i zalety takiego rozwiązania?\n",
    "- Student podczas treningu sieci neuronowej zauważył, że wyniki funkcji celu w czasie optymalizacji modelu nie są stabilne. Spodziewa się, że zwiększenie rozmiaru paczki danych przyczyni się do bardziej stabilnego treningu sieci i poprawi wyniki. Jednakże, dalsze zwiększenie paczki danych nie jest możliwe gdyż nie mieści się ona w pamięci na karcie GPU. Sama implementacja sieci i reprezentacji zbioru danych jest wg. studenta optymalna, inna karta GPU o większej pamięci nie jest dostępna. Jak rozwiązac ten problem? Podaj zarys implementacji. \n",
    "\n",
    "\n",
    "Odpowiedź na ostatnią kropkę umieść poniżej.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Odpowiedź - Należy dokonać chunkowania zbioru danych wkładanego na kartę. A gradient zbierać, w efekcie tworzyć mini batchowanie."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m StepInterval \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m epochs:\n\u001B[1;32m----> 8\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m (i, (x, y)) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[43mbatches\u001B[49m):\n\u001B[0;32m      9\u001B[0m     prediction \u001B[38;5;241m=\u001B[39m model(x)\n\u001B[0;32m     10\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_function(prediction, y) \u001B[38;5;241m/\u001B[39m StepInterval\n",
      "\u001B[1;31mNameError\u001B[0m: name 'batches' is not defined"
     ]
    }
   ],
   "source": [
    "# Coś takiego\n",
    "from typing import Any\n",
    "\n",
    "epochs: range = range(5)\n",
    "batches: list[list[Any]]\n",
    "StepInterval = 10\n",
    "for epoch in epochs:\n",
    "  for (i, (x, y)) in enumerate(batches):\n",
    "    prediction = model(x)\n",
    "    loss = loss_function(prediction, y) / StepInterval\n",
    "    ...\n",
    "\n",
    "    if i % StepInterval == 0:\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T18:54:10.547707600Z",
     "start_time": "2024-01-10T18:54:10.473191900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3\n",
    "Celem ostatniego ćwiczenia jest rozszerzenie modelu o warstwę CRF (nie można korzystać z gotowców). Implementacja:\n",
    "- powinna operować tylko na jednej sekwencji (brak obsługi paczek danych w programowaniu dynamicznym)\n",
    "- powinna być stabilna numerycznie tj. operować na logarytmach prawdopodobieństw (patrz rozdział \"Uwaga implementacyjna\" w wykładzie o CRF)\n",
    "- warstwa CRF powinna być zaimplementowana wg. schematu \"rozbicie funkcji oceny na ocenę emisji i tranzycji\" $$\\Psi_i (y_i, y_{i-1}, \\vec{x}) = \\exp\\left(\n",
    " {W}_{y_i}h_i\\right) \\exp\\left( \\vec{b}_{y_i, y_{i-1}} \\right)\n",
    " $$\n",
    " gdzie ${W}_{y_i}h_i$ to ocena kompatybilności emisji, a $\\vec{b}_{y_i, y_{i-1}}$ to kompatybilność tranzycji. Reprezentacja $h_i$ jest obliczana poprzez sieć rekurenycjną LSTM. Zwróć więc uwagę, że ${W}_{y_i}h_i$ to wynik warstwy liniowej mającej tyle neuronów wyjściowych ile tagów, za wejście przyjmując stan ukryty sieci rekurencyjnej. Z kolei $\\vec{b}_{y_i, y_{i-1}}$ to jedna waga reprezentująca ocenę tranzycji, która jest dodatkowym parametrem warstwy CRF. Potrzebujemy więc macierz takich wag o wymiarach $C\\times C$ gdzie C to liczba tagów. \n",
    "- Model powinien uwzględniać też reprezentacje tagów `START` i `STOP`. W rezultacie więc wygodnie zaimplementować macierz parametrów oceniających tranzycję jako macierz o wymiarach $C+2\\times C+2$.\n",
    " \n",
    "W czasie treningu modelu powinieneś maksymalizować logarytmiczną funkcję wiarygodności, co przy paczce danych o wielkości 1 sprowadza się do maksymalizacji logarytmu prawdopodobieństwa prawidłowej sekwencji. \n",
    "$$\\max \\log P({y} | {x}) = \\log \\frac{\\prod_{i=1}^n \\Psi_i (y_i, y_{i-1}, {x})}{Z({x})}  = \\sum_{i=1}^n  \\log \\Psi_i (y_i, y_{i-1}, {x}) - \\log Z({x})  $$\n",
    "W PyTorch algorytmy optymalizacyjne minimalizują funkcje celu, więc wynik przed policzeniem gradientów musisz pomnożyć przez $-1$.\n",
    "\n",
    "Do dyspozycji masz funkcję `log_sum_exp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:54:26.090920700Z",
     "start_time": "2024-01-10T18:54:26.087599Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(vec):\n",
    "  max_score = torch.max(vec)\n",
    "  return max_score + torch.log(torch.sum(torch.exp(vec - max_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uzupełnij klasę `TaggerCRFNet` takby aby implementowała powyższą architekturę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:54:27.280425800Z",
     "start_time": "2024-01-10T18:54:27.274192300Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "class TaggerCRFNet(nn.Module):\n",
    "  def __init__(self, vocab_size, hidden_size, n_tags):\n",
    "    super(TaggerCRFNet, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, WordEmbedding)\n",
    "    self.lstm = nn.LSTM(WordEmbedding, hidden_size, bidirectional=True)\n",
    "    self.fc = nn.Linear(hidden_size * 2, n_tags)\n",
    "    self.n_tags = n_tags\n",
    "    self.START_TAG = self.n_tags\n",
    "    self.STOP_TAG = self.n_tags + 1\n",
    "    # TWÓJ KOD TUTAJ\n",
    "    # self.transitions = \n",
    "    # Macierz przechowująca oceny tranzycji o wymiarach (n_tags+2,n_tags+2). \n",
    "    # Dodatkowe tensory parametrów modelu aby były uwzględniane przez \n",
    "    # model.parameters() muszą być opakowane obiektem nn.Parameter().\n",
    "    self.transitions = nn.Parameter(torch.randn(n_tags + 2, n_tags + 2))\n",
    "\n",
    "  def _get_features(self, sentence, seq_lengths):\n",
    "    \"\"\"\n",
    "    Funkcja ekstrakcji cech przez sieć rekurencyjną LSTM.\n",
    "    Wyjście sieci zwraca od razu ocenę emisji każdego taga W_{y_i}*h_i\n",
    "    \n",
    "    Zwróć uwagę na wywołanie .view() na wyniku funkcji. Wynik ma \n",
    "    wymiarowość długość sekwencji x liczba tagów (brak obsługi kilku sekwencji na raz)\n",
    "    \"\"\"\n",
    "    x = self.embedding(sentence)\n",
    "    x = pack_padded_sequence(x, seq_lengths)\n",
    "    x, _ = self.lstm(x)\n",
    "    x, _ = pad_packed_sequence(x)\n",
    "    x = self.fc(x)\n",
    "    return x.view(-1, self.n_tags)\n",
    "\n",
    "  def _get_numerator(self, features, tags):\n",
    "    \"\"\"\n",
    "    Funkcja przyjmuje na wejście features zwracane przez _get_features\n",
    "    oraz sekwencję tagów. Obliczany jest (zlogarytmowany) licznik prawdopodobieństwa \n",
    "    podanej sekwencji. (Pierwszy term podanej wyżej funkcji celu)\n",
    "    \n",
    "    Zwróć uwagę w jaki sposób jest wykorzystywana macierz transitions.\n",
    "    Pierwszym indeksem jest *kolejny* tag\n",
    "    \"\"\"\n",
    "    score = torch.zeros(1)\n",
    "    # Funkcje kompatybilności emisji\n",
    "    for i, tag in enumerate(tags):\n",
    "      score = score + features[i, tag]\n",
    "    # Funkcje kompatybilności tranzycji\n",
    "    score = score + self.transitions[tags[0], self.START_TAG]\n",
    "    for i in range(len(tags) - 1):\n",
    "      score = score + self.transitions[tags[i + 1], tags[i]]\n",
    "    score = score + self.transitions[self.STOP_TAG, tags[-1]]\n",
    "    return score\n",
    "\n",
    "  def _forward_alg(self, features, seq_lengths):\n",
    "    \"\"\"\n",
    "    Funkcja zwracająca logarytm sumy statystycznej (drugi term funkcji celu)\n",
    "    \"\"\"\n",
    "    forwarded = features[0] + self.transitions[:self.n_tags, self.START_TAG].view(-1)\n",
    "    transitions = self.transitions[:self.n_tags, :self.n_tags]\n",
    "\n",
    "    for feature in map(lambda feature: feature.unsqueeze(1) + transitions, features[1:]):\n",
    "      out = forwarded + feature\n",
    "      max = out.max(1).values\n",
    "\n",
    "      forwarded = max + (out - max.unsqueeze(1)).logsumexp(1)\n",
    "\n",
    "    return log_sum_exp(forwarded + self.transitions[self.STOP_TAG, :self.n_tags])\n",
    "\n",
    "  def seq_log_probability(self, sentence, tags, seq_lengths):\n",
    "    \"\"\"\n",
    "    Funkcja zwracająca logarytm prawdopodobieństwa podanej sekwencji\n",
    "    \"\"\"\n",
    "    features = self._get_features(sentence, seq_lengths)\n",
    "    numerator = self._get_numerator(features, tags)\n",
    "    partition_function = self._forward_alg(features, seq_lengths)\n",
    "    return numerator - partition_function\n",
    "\n",
    "  def forward(self, sentence, seq_lengths):\n",
    "    \"\"\"\n",
    "    Funkcja zwracająca najbardziej prawdopodobną sekwencję tagów dla podanego wejścia.\n",
    "    \n",
    "    Porada: zaimplementuj ją w drugiej kolejności, jak seq_log_probability będzie prawidłowo działać\n",
    "    \"\"\"\n",
    "    features = self._get_features(sentence, seq_lengths)\n",
    "    transitions = self.transitions[:self.n_tags, :self.n_tags]\n",
    "\n",
    "    memo = []\n",
    "    forwarded = features[0] + self.transitions[:self.n_tags, self.START_TAG].view(-1)\n",
    "    for feature in map(lambda feature: feature.unsqueeze(1) + transitions, features[1:]):\n",
    "      forwarded, tags = (forwarded + feature).max(1)\n",
    "      memo.insert(0, tags)\n",
    "\n",
    "    forwarded += self.transitions[self.STOP_TAG, :self.n_tags]\n",
    "    tag = forwarded.max(0).indices\n",
    "\n",
    "    tags = [tag]\n",
    "    for mem in memo: tags.insert(0, tag := mem[tag])\n",
    "\n",
    "    return tags\n",
    "\n",
    "  def score_seq(self, sentence, tags, seq_lengths):\n",
    "    \"\"\"\n",
    "    Funkcja dla testów implementacji.\n",
    "    Wyznacz ocenę sekwencji (zlogarytmowany licznik). \n",
    "    \"\"\"\n",
    "    features = self._get_features(sentence, seq_lengths)\n",
    "    numerator = self._get_numerator(features, tags)\n",
    "    return numerator\n",
    "\n",
    "  def partition_function(self, sentence, tags, seq_lengths):\n",
    "    \"\"\"\n",
    "    Funkcja dla testów implementacji.\n",
    "    Wyznacz sumę statystyczną (zlogarytmowany mianownik). \n",
    "    \"\"\"\n",
    "    features = self._get_features(sentence, seq_lengths)\n",
    "    gold_score = self._forward_alg(features, seq_lengths)\n",
    "    return gold_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poprawność algorytmu \"w przód i w tył\" mozna prosto sprawdzić, porównując wynik z wynikiem algorytmu zachłannego (tj. iterującym po wszystkich sekwencjach), dla krótkiej sekwencji z małą liczbą możliwych tagów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:54:30.572353300Z",
     "start_time": "2024-01-10T18:54:29.463989200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programowanie dynamiczne tensor(3.3924, grad_fn=<AddBackward0>)\n",
      "Algorytm zachłanny tensor(3.3924)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "data = torch.tensor([1, 2, 5, 10, 11, 3]).view(6, 1)\n",
    "tags = torch.tensor([1, 2, 0, 1, 1, 1]).view(6, 1)\n",
    "length = torch.tensor([6])\n",
    "\n",
    "model = TaggerCRFNet(vocab_size=12, hidden_size=3, n_tags=3)\n",
    "sum_forward_alg = model.partition_function(data, tags, length)\n",
    "print('Programowanie dynamiczne', sum_forward_alg)\n",
    "sum_score = []\n",
    "for i in itertools.product(range(3), repeat=6):\n",
    "  new_tags = torch.tensor(i).view(6, 1)\n",
    "  sum_score.append(model.score_seq(data, new_tags, length))\n",
    "print(\"Algorytm zachłanny\", log_sum_exp(torch.tensor(sum_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatecznie zaimplementuj trening modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T18:56:51.365986800Z",
     "start_time": "2024-01-10T18:54:31.201164100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function:  tensor([109.2652])\n",
      "Loss function:  tensor(81.0574)\n",
      "Loss function:  tensor(86.9654)\n",
      "Loss function:  tensor(75.7202)\n",
      "Loss function:  tensor(57.6637)\n",
      "Loss function:  tensor(91.6769)\n",
      "Loss function:  tensor(69.5914)\n",
      "Loss function:  tensor(71.2319)\n",
      "Loss function:  tensor(73.8273)\n",
      "Loss function:  tensor(58.4193)\n",
      "Loss function:  tensor(55.3911)\n",
      "Loss function:  tensor(44.2150)\n",
      "Loss function:  tensor(53.9330)\n",
      "Loss function:  tensor(52.7270)\n",
      "Loss function:  tensor(46.1000)\n",
      "Loss function:  tensor(58.1001)\n",
      "Loss function:  tensor(22.2074)\n",
      "Loss function:  tensor(51.1980)\n",
      "Loss function:  tensor(40.8207)\n",
      "Loss function:  tensor(39.2408)\n",
      "Loss function:  tensor(61.6182)\n",
      "Loss function:  tensor(60.0108)\n",
      "Loss function:  tensor(58.5081)\n",
      "Loss function:  tensor(40.0116)\n",
      "Loss function:  tensor(31.9350)\n",
      "Loss function:  tensor(34.0109)\n",
      "Loss function:  tensor(37.4055)\n",
      "Loss function:  tensor(50.1884)\n",
      "Loss function:  tensor(45.7366)\n",
      "Loss function:  tensor(40.4769)\n",
      "Loss function:  tensor(43.0692)\n",
      "Loss function:  tensor(41.9671)\n",
      "Loss function:  tensor(37.1719)\n",
      "Loss function:  tensor(43.8448)\n",
      "Loss function:  tensor(38.9084)\n",
      "Loss function:  tensor(31.8951)\n",
      "Loss function:  tensor(35.7045)\n",
      "Loss function:  tensor(27.7960)\n",
      "Loss function:  tensor(29.7394)\n",
      "Loss function:  tensor(24.4822)\n",
      "Loss function:  tensor(27.9403)\n",
      "Loss function:  tensor(32.1805)\n",
      "Loss function:  tensor(34.2315)\n",
      "Loss function:  tensor(31.0398)\n",
      "Loss function:  tensor(41.2119)\n",
      "Loss function:  tensor(31.1198)\n",
      "Loss function:  tensor(20.6900)\n",
      "Loss function:  tensor(34.5171)\n",
      "Loss function:  tensor(47.6311)\n",
      "Loss function:  tensor(33.3879)\n",
      "Loss function:  tensor(50.1120)\n",
      "Loss function:  tensor(33.9449)\n",
      "Loss function:  tensor(43.1754)\n",
      "Loss function:  tensor(41.0862)\n",
      "Loss function:  tensor(30.2047)\n",
      "Loss function:  tensor(34.6641)\n",
      "Loss function:  tensor(28.0056)\n",
      "Loss function:  tensor(33.7178)\n",
      "Loss function:  tensor(25.3678)\n",
      "Loss function:  tensor(18.0272)\n",
      "Loss function:  tensor(16.1735)\n",
      "Loss function:  tensor(38.8280)\n",
      "Loss function:  tensor(23.7297)\n",
      "Loss function:  tensor(29.2490)\n",
      "Loss function:  tensor(33.2388)\n",
      "Loss function:  tensor(31.7318)\n",
      "Loss function:  tensor(22.8394)\n",
      "Loss function:  tensor(27.4990)\n",
      "Loss function:  tensor(22.0905)\n",
      "Loss function:  tensor(39.0440)\n",
      "Loss function:  tensor(33.1062)\n",
      "Loss function:  tensor(30.3986)\n",
      "Loss function:  tensor(27.2624)\n",
      "Loss function:  tensor(27.3623)\n",
      "Loss function:  tensor(36.1000)\n",
      "Loss function:  tensor(32.4456)\n",
      "Loss function:  tensor(32.4907)\n",
      "Loss function:  tensor(33.7263)\n",
      "Loss function:  tensor(38.3542)\n",
      "Loss function:  tensor(26.0653)\n",
      "Loss function:  tensor(18.9758)\n",
      "Loss function:  tensor(25.4791)\n",
      "Loss function:  tensor(25.0811)\n",
      "Loss function:  tensor(27.1471)\n",
      "Loss function:  tensor(27.1624)\n",
      "Loss function:  tensor(26.1576)\n",
      "Loss function:  tensor(26.6621)\n",
      "Loss function:  tensor(20.4741)\n",
      "Loss function:  tensor(27.9148)\n",
      "Loss function:  tensor(24.3521)\n",
      "Loss function:  tensor(32.0673)\n",
      "Loss function:  tensor(35.5849)\n",
      "Loss function:  tensor(39.2624)\n",
      "Loss function:  tensor(25.6035)\n",
      "Loss function:  tensor(27.2927)\n",
      "Loss function:  tensor(20.3007)\n",
      "Loss function:  tensor(22.7428)\n",
      "Loss function:  tensor(25.4667)\n",
      "Loss function:  tensor(19.1370)\n",
      "Loss function:  tensor(25.9492)\n",
      "Loss function:  tensor(23.1900)\n",
      "Loss function:  tensor(26.1661)\n",
      "Loss function:  tensor(34.8501)\n",
      "Loss function:  tensor(29.3145)\n",
      "Loss function:  tensor(26.7285)\n",
      "Loss function:  tensor(25.1662)\n",
      "Loss function:  tensor(19.9038)\n",
      "Loss function:  tensor(25.5794)\n",
      "Loss function:  tensor(26.4533)\n",
      "Loss function:  tensor(27.6040)\n",
      "Loss function:  tensor(19.4296)\n",
      "Loss function:  tensor(26.9017)\n",
      "Loss function:  tensor(24.1305)\n",
      "Loss function:  tensor(21.4463)\n",
      "Loss function:  tensor(34.1783)\n",
      "Loss function:  tensor(20.5550)\n",
      "Loss function:  tensor(35.3371)\n",
      "Loss function:  tensor(17.9614)\n",
      "Loss function:  tensor(33.5411)\n",
      "Loss function:  tensor(30.0972)\n",
      "Loss function:  tensor(38.6139)\n",
      "Loss function:  tensor(26.8707)\n",
      "Loss function:  tensor(23.6129)\n",
      "Loss function:  tensor(33.4281)\n",
      "Loss function:  tensor(19.2588)\n",
      "Loss function:  tensor(16.5691)\n",
      "Loss function:  tensor(25.8687)\n",
      "Loss function:  tensor(15.3115)\n",
      "Loss function:  tensor(14.8542)\n",
      "Loss function:  tensor(16.2114)\n",
      "Loss function:  tensor(29.7099)\n",
      "Loss function:  tensor(25.0408)\n",
      "Loss function:  tensor(25.2770)\n",
      "Loss function:  tensor(14.9485)\n",
      "Loss function:  tensor(20.3961)\n",
      "Loss function:  tensor(21.3492)\n",
      "Loss function:  tensor(23.0522)\n",
      "Loss function:  tensor(19.1517)\n",
      "Loss function:  tensor(26.7681)\n",
      "Loss function:  tensor(24.9250)\n",
      "Loss function:  tensor(16.6176)\n",
      "Loss function:  tensor(20.4285)\n",
      "Loss function:  tensor(18.8883)\n",
      "Loss function:  tensor(26.7077)\n",
      "Loss function:  tensor(34.7222)\n",
      "Loss function:  tensor(32.4178)\n",
      "Loss function:  tensor(23.6352)\n",
      "Loss function:  tensor(28.5308)\n",
      "Loss function:  tensor(22.4759)\n",
      "Loss function:  tensor(13.8829)\n",
      "Loss function:  tensor(23.7882)\n",
      "Loss function:  tensor(16.7023)\n",
      "Loss function:  tensor(29.1736)\n",
      "Loss function:  tensor(17.1651)\n",
      "Loss function:  tensor(17.6869)\n",
      "Loss function:  tensor(21.3626)\n",
      "Loss function:  tensor(22.9321)\n",
      "Loss function:  tensor(16.1489)\n",
      "Loss function:  tensor(17.3635)\n",
      "Loss function:  tensor(35.6164)\n",
      "Loss function:  tensor(29.2380)\n",
      "Loss function:  tensor(34.7475)\n",
      "Loss function:  tensor(23.3911)\n",
      "Loss function:  tensor(26.9720)\n",
      "Loss function:  tensor(24.9633)\n",
      "Loss function:  tensor(30.8161)\n",
      "Loss function:  tensor(27.2533)\n",
      "Loss function:  tensor(16.8899)\n",
      "Loss function:  tensor(22.2137)\n",
      "Loss function:  tensor(12.1196)\n",
      "Loss function:  tensor(15.5411)\n",
      "Loss function:  tensor(19.1949)\n",
      "Loss function:  tensor(14.3997)\n",
      "Loss function:  tensor(15.2890)\n",
      "Loss function:  tensor(18.4948)\n",
      "Loss function:  tensor(11.0412)\n",
      "Loss function:  tensor(11.6501)\n",
      "Loss function:  tensor(25.6311)\n",
      "Loss function:  tensor(26.2571)\n",
      "Loss function:  tensor(21.6760)\n",
      "Loss function:  tensor(19.1466)\n",
      "Loss function:  tensor(15.5334)\n",
      "Loss function:  tensor(18.1951)\n",
      "Loss function:  tensor(17.1488)\n",
      "Loss function:  tensor(17.6172)\n",
      "Loss function:  tensor(19.8405)\n",
      "Loss function:  tensor(12.8421)\n",
      "Loss function:  tensor(18.0412)\n",
      "Loss function:  tensor(16.3712)\n",
      "Loss function:  tensor(9.2586)\n",
      "Loss function:  tensor(14.3465)\n",
      "Loss function:  tensor(19.2479)\n",
      "Loss function:  tensor(17.6717)\n",
      "Loss function:  tensor(17.9490)\n",
      "Loss function:  tensor(17.9450)\n",
      "Loss function:  tensor(16.3084)\n",
      "Loss function:  tensor(24.8366)\n",
      "Loss function:  tensor(23.3855)\n",
      "Loss function:  tensor(14.7818)\n",
      "Loss function:  tensor(19.4549)\n",
      "Loss function:  tensor(17.1999)\n",
      "Loss function:  tensor(14.6438)\n",
      "Loss function:  tensor(16.0855)\n",
      "Loss function:  tensor(18.3753)\n",
      "Loss function:  tensor(11.8879)\n",
      "Loss function:  tensor(15.5710)\n",
      "Loss function:  tensor(17.4622)\n",
      "Loss function:  tensor(13.3500)\n",
      "Loss function:  tensor(11.5111)\n",
      "Loss function:  tensor(10.8865)\n",
      "Loss function:  tensor(14.6436)\n",
      "Loss function:  tensor(15.3179)\n",
      "Loss function:  tensor(17.0588)\n",
      "Loss function:  tensor(30.1682)\n",
      "Loss function:  tensor(20.9354)\n",
      "Loss function:  tensor(20.3383)\n",
      "Loss function:  tensor(14.9296)\n",
      "Loss function:  tensor(12.2721)\n",
      "Loss function:  tensor(17.0966)\n",
      "Loss function:  tensor(10.4672)\n",
      "Loss function:  tensor(7.6163)\n",
      "Loss function:  tensor(32.9768)\n",
      "Loss function:  tensor(17.0523)\n",
      "Loss function:  tensor(11.5041)\n",
      "Loss function:  tensor(12.9334)\n",
      "Loss function:  tensor(14.9393)\n",
      "Loss function:  tensor(12.1651)\n",
      "Loss function:  tensor(10.9429)\n",
      "Loss function:  tensor(12.5450)\n",
      "Loss function:  tensor(11.5037)\n",
      "Loss function:  tensor(11.4421)\n",
      "Loss function:  tensor(16.9969)\n",
      "Loss function:  tensor(11.5377)\n",
      "Loss function:  tensor(14.6540)\n",
      "Loss function:  tensor(21.2169)\n",
      "Loss function:  tensor(14.4917)\n",
      "Loss function:  tensor(15.9414)\n",
      "Loss function:  tensor(9.1989)\n",
      "Loss function:  tensor(10.4164)\n",
      "Loss function:  tensor(8.3716)\n",
      "Loss function:  tensor(17.0704)\n",
      "Loss function:  tensor(17.1983)\n",
      "Loss function:  tensor(12.7305)\n",
      "Loss function:  tensor(15.0918)\n",
      "Loss function:  tensor(17.0122)\n",
      "Loss function:  tensor(15.0872)\n",
      "Loss function:  tensor(17.1038)\n",
      "Loss function:  tensor(13.3190)\n",
      "Loss function:  tensor(18.5901)\n",
      "Loss function:  tensor(25.7258)\n",
      "Loss function:  tensor(23.3360)\n",
      "Loss function:  tensor(14.0629)\n",
      "Loss function:  tensor(14.5408)\n",
      "Loss function:  tensor(12.5202)\n",
      "Loss function:  tensor(16.9293)\n",
      "Loss function:  tensor(15.4375)\n",
      "Loss function:  tensor(17.7950)\n",
      "Loss function:  tensor(11.7201)\n",
      "Loss function:  tensor(13.7193)\n",
      "Loss function:  tensor(19.9604)\n",
      "Loss function:  tensor(15.9135)\n",
      "Loss function:  tensor(21.5145)\n",
      "Loss function:  tensor(21.1574)\n",
      "Loss function:  tensor(21.8879)\n",
      "Loss function:  tensor(12.6293)\n",
      "Loss function:  tensor(16.8189)\n",
      "Loss function:  tensor(12.1090)\n",
      "Loss function:  tensor(20.1047)\n",
      "Loss function:  tensor(20.2084)\n",
      "Loss function:  tensor(15.9471)\n",
      "Loss function:  tensor(13.1402)\n",
      "Loss function:  tensor(16.2597)\n",
      "Loss function:  tensor(11.4272)\n",
      "Loss function:  tensor(23.4439)\n",
      "Loss function:  tensor(17.8395)\n",
      "Loss function:  tensor(22.9808)\n",
      "Loss function:  tensor(27.5922)\n",
      "Loss function:  tensor(16.8464)\n",
      "Loss function:  tensor(20.5298)\n",
      "Loss function:  tensor(12.5406)\n",
      "Loss function:  tensor(21.8061)\n",
      "Loss function:  tensor(21.1174)\n",
      "Loss function:  tensor(15.8552)\n",
      "Loss function:  tensor(18.9835)\n",
      "Loss function:  tensor(14.4733)\n",
      "Loss function:  tensor(12.6556)\n",
      "Loss function:  tensor(18.7086)\n",
      "Loss function:  tensor(16.6145)\n",
      "Loss function:  tensor(14.2737)\n",
      "Loss function:  tensor(12.6914)\n",
      "Loss function:  tensor(8.1359)\n",
      "Loss function:  tensor(23.0428)\n",
      "Loss function:  tensor(16.4817)\n",
      "Loss function:  tensor(20.9183)\n",
      "Loss function:  tensor(16.8318)\n",
      "Loss function:  tensor(18.5296)\n",
      "Loss function:  tensor(18.5606)\n",
      "Loss function:  tensor(18.6782)\n",
      "Loss function:  tensor(18.9805)\n",
      "Loss function:  tensor(20.4377)\n",
      "Loss function:  tensor(15.5334)\n",
      "Loss function:  tensor(13.7562)\n",
      "Loss function:  tensor(14.8760)\n",
      "Loss function:  tensor(14.3165)\n",
      "Loss function:  tensor(20.1133)\n",
      "Loss function:  tensor(12.3702)\n",
      "Loss function:  tensor(12.2054)\n",
      "Loss function:  tensor(16.3021)\n",
      "Loss function:  tensor(14.8991)\n",
      "Loss function:  tensor(11.7829)\n",
      "Loss function:  tensor(9.0231)\n",
      "Loss function:  tensor(10.2918)\n",
      "Loss function:  tensor(9.6817)\n",
      "Loss function:  tensor(10.9768)\n",
      "Loss function:  tensor(21.3058)\n",
      "Loss function:  tensor(12.0200)\n",
      "Loss function:  tensor(20.2631)\n",
      "Loss function:  tensor(17.6650)\n",
      "Loss function:  tensor(16.5596)\n",
      "Loss function:  tensor(18.4911)\n",
      "Loss function:  tensor(15.5134)\n",
      "Loss function:  tensor(19.2292)\n",
      "Loss function:  tensor(19.9679)\n",
      "Loss function:  tensor(15.3806)\n",
      "Loss function:  tensor(9.3025)\n",
      "Loss function:  tensor(12.5640)\n",
      "Loss function:  tensor(10.5189)\n",
      "Loss function:  tensor(11.8856)\n",
      "Loss function:  tensor(19.9327)\n",
      "Loss function:  tensor(16.1708)\n",
      "Loss function:  tensor(10.4550)\n",
      "Loss function:  tensor(14.3490)\n",
      "Loss function:  tensor(19.2926)\n",
      "Loss function:  tensor(13.1396)\n",
      "Loss function:  tensor(10.6988)\n",
      "Loss function:  tensor(9.5469)\n",
      "Loss function:  tensor(17.8289)\n",
      "Loss function:  tensor(15.9660)\n",
      "Loss function:  tensor(15.7954)\n",
      "Loss function:  tensor(12.8166)\n",
      "Loss function:  tensor(17.6870)\n",
      "Loss function:  tensor(13.3070)\n",
      "Loss function:  tensor(8.3440)\n",
      "Loss function:  tensor(10.7112)\n",
      "Loss function:  tensor(10.3579)\n",
      "Loss function:  tensor(15.3894)\n",
      "Loss function:  tensor(9.6432)\n",
      "Loss function:  tensor(8.9932)\n",
      "Loss function:  tensor(18.4420)\n",
      "Loss function:  tensor(15.2798)\n",
      "Loss function:  tensor(13.6861)\n",
      "Loss function:  tensor(12.9947)\n",
      "Loss function:  tensor(9.4780)\n",
      "Loss function:  tensor(10.0014)\n",
      "Loss function:  tensor(14.0784)\n",
      "Loss function:  tensor(9.3157)\n",
      "Loss function:  tensor(12.3331)\n",
      "Loss function:  tensor(8.2426)\n",
      "Loss function:  tensor(17.9538)\n",
      "Loss function:  tensor(6.8173)\n",
      "Loss function:  tensor(12.3202)\n",
      "Loss function:  tensor(8.8913)\n",
      "Loss function:  tensor(15.0479)\n",
      "Loss function:  tensor(10.9158)\n",
      "Loss function:  tensor(16.5051)\n",
      "Loss function:  tensor(14.2175)\n",
      "Loss function:  tensor(13.0821)\n",
      "Loss function:  tensor(11.1875)\n",
      "Loss function:  tensor(10.9164)\n",
      "Loss function:  tensor(13.2756)\n",
      "Loss function:  tensor(12.1726)\n",
      "Loss function:  tensor(11.0676)\n",
      "Loss function:  tensor(11.8791)\n",
      "Loss function:  tensor(15.6949)\n",
      "Loss function:  tensor(12.9467)\n",
      "Loss function:  tensor(10.9981)\n",
      "Loss function:  tensor(17.0279)\n",
      "Loss function:  tensor(8.9058)\n",
      "Loss function:  tensor(4.7851)\n",
      "Loss function:  tensor(8.6935)\n",
      "Loss function:  tensor(8.4122)\n",
      "Loss function:  tensor(9.7656)\n",
      "Loss function:  tensor(6.9012)\n",
      "Loss function:  tensor(6.7581)\n",
      "Loss function:  tensor(14.3805)\n",
      "Loss function:  tensor(14.8967)\n",
      "Loss function:  tensor(17.2492)\n",
      "Loss function:  tensor(11.0375)\n",
      "Loss function:  tensor(20.2952)\n",
      "Loss function:  tensor(21.3645)\n",
      "Loss function:  tensor(9.2648)\n",
      "Loss function:  tensor(7.2555)\n",
      "Loss function:  tensor(15.0529)\n",
      "Loss function:  tensor(15.1611)\n",
      "Loss function:  tensor(16.4175)\n",
      "Loss function:  tensor(10.1287)\n",
      "Loss function:  tensor(10.7342)\n",
      "Loss function:  tensor(15.5147)\n",
      "Loss function:  tensor(10.7757)\n",
      "Loss function:  tensor(11.8402)\n",
      "Loss function:  tensor(10.9805)\n",
      "Loss function:  tensor(8.6568)\n",
      "Loss function:  tensor(8.5233)\n",
      "Loss function:  tensor(7.5829)\n",
      "Loss function:  tensor(12.7789)\n",
      "Loss function:  tensor(11.4219)\n",
      "Loss function:  tensor(10.5180)\n",
      "Loss function:  tensor(10.1515)\n",
      "Loss function:  tensor(13.3418)\n",
      "Loss function:  tensor(16.7358)\n",
      "Loss function:  tensor(10.5435)\n",
      "Loss function:  tensor(10.5131)\n",
      "Loss function:  tensor(17.5347)\n",
      "Loss function:  tensor(12.5613)\n",
      "Loss function:  tensor(11.6824)\n",
      "Loss function:  tensor(8.4937)\n",
      "Loss function:  tensor(13.6091)\n",
      "Loss function:  tensor(13.1961)\n",
      "Loss function:  tensor(9.2127)\n",
      "Loss function:  tensor(14.2182)\n",
      "Loss function:  tensor(10.2006)\n",
      "Loss function:  tensor(11.2935)\n",
      "Loss function:  tensor(5.8954)\n",
      "Loss function:  tensor(10.0792)\n",
      "Loss function:  tensor(12.2234)\n",
      "Loss function:  tensor(15.7506)\n",
      "Loss function:  tensor(12.1022)\n",
      "Loss function:  tensor(12.6144)\n",
      "Loss function:  tensor(20.3354)\n",
      "Loss function:  tensor(12.0366)\n",
      "Loss function:  tensor(17.3357)\n",
      "Loss function:  tensor(11.9649)\n",
      "Loss function:  tensor(13.9078)\n",
      "Loss function:  tensor(16.7413)\n",
      "Loss function:  tensor(17.7308)\n",
      "Loss function:  tensor(9.3577)\n",
      "Loss function:  tensor(6.1995)\n",
      "Loss function:  tensor(7.9201)\n",
      "Loss function:  tensor(18.4809)\n",
      "Loss function:  tensor(9.9990)\n",
      "Loss function:  tensor(9.9822)\n",
      "Loss function:  tensor(10.8344)\n",
      "Loss function:  tensor(17.2829)\n",
      "Loss function:  tensor(12.6105)\n",
      "Loss function:  tensor(18.6304)\n",
      "Loss function:  tensor(15.4742)\n",
      "Loss function:  tensor(12.0492)\n",
      "Loss function:  tensor(3.6755)\n",
      "Loss function:  tensor(10.6501)\n",
      "Loss function:  tensor(11.7080)\n",
      "Loss function:  tensor(12.8392)\n",
      "Loss function:  tensor(15.9068)\n",
      "Loss function:  tensor(15.6984)\n",
      "Loss function:  tensor(10.7004)\n",
      "Loss function:  tensor(9.3732)\n",
      "Loss function:  tensor(13.1183)\n",
      "Loss function:  tensor(12.9958)\n",
      "Loss function:  tensor(11.6707)\n",
      "Loss function:  tensor(12.4605)\n",
      "Loss function:  tensor(15.1010)\n",
      "Loss function:  tensor(13.6415)\n",
      "Loss function:  tensor(14.1974)\n",
      "Loss function:  tensor(14.7657)\n",
      "Loss function:  tensor(10.3961)\n",
      "Loss function:  tensor(11.6629)\n",
      "Loss function:  tensor(11.7777)\n",
      "Loss function:  tensor(9.4552)\n",
      "Loss function:  tensor(9.3708)\n",
      "Loss function:  tensor(8.7038)\n",
      "Loss function:  tensor(7.6483)\n",
      "Loss function:  tensor(8.2902)\n",
      "Loss function:  tensor(9.2862)\n",
      "Loss function:  tensor(8.3361)\n",
      "Loss function:  tensor(10.1046)\n",
      "Loss function:  tensor(28.1108)\n",
      "Loss function:  tensor(17.4967)\n",
      "Loss function:  tensor(17.2588)\n",
      "Loss function:  tensor(22.1309)\n",
      "Loss function:  tensor(15.2690)\n",
      "Loss function:  tensor(14.0116)\n",
      "Loss function:  tensor(15.6679)\n",
      "Loss function:  tensor(20.4741)\n",
      "Loss function:  tensor(13.5904)\n",
      "Loss function:  tensor(13.3787)\n",
      "Loss function:  tensor(7.9465)\n",
      "Loss function:  tensor(12.7979)\n",
      "Loss function:  tensor(11.6114)\n",
      "Loss function:  tensor(14.4681)\n",
      "Loss function:  tensor(10.8695)\n",
      "Loss function:  tensor(11.4515)\n",
      "Loss function:  tensor(10.7350)\n",
      "Loss function:  tensor(13.0246)\n",
      "Loss function:  tensor(13.2229)\n",
      "Loss function:  tensor(13.9714)\n",
      "Loss function:  tensor(8.0231)\n",
      "Loss function:  tensor(8.6966)\n",
      "Loss function:  tensor(12.8018)\n",
      "Loss function:  tensor(10.9739)\n",
      "Loss function:  tensor(9.2670)\n",
      "Loss function:  tensor(14.4603)\n",
      "Loss function:  tensor(16.1272)\n",
      "Loss function:  tensor(5.6324)\n",
      "Loss function:  tensor(9.6389)\n",
      "Loss function:  tensor(15.1982)\n",
      "Loss function:  tensor(10.1579)\n",
      "Loss function:  tensor(7.3213)\n",
      "Loss function:  tensor(6.7523)\n",
      "Loss function:  tensor(12.2734)\n",
      "Loss function:  tensor(15.3441)\n",
      "Loss function:  tensor(6.5281)\n",
      "Loss function:  tensor(4.7307)\n",
      "Loss function:  tensor(10.3481)\n",
      "Loss function:  tensor(12.9966)\n",
      "Loss function:  tensor(9.2137)\n",
      "Loss function:  tensor(7.5913)\n",
      "Loss function:  tensor(12.2829)\n",
      "Loss function:  tensor(12.2061)\n",
      "Loss function:  tensor(13.9293)\n",
      "Loss function:  tensor(11.9532)\n",
      "Loss function:  tensor(11.9336)\n",
      "Loss function:  tensor(10.1734)\n",
      "Loss function:  tensor(11.3992)\n",
      "Loss function:  tensor(7.9060)\n",
      "Loss function:  tensor(8.7286)\n",
      "Loss function:  tensor(9.2267)\n",
      "Loss function:  tensor(8.0905)\n",
      "Loss function:  tensor(4.6816)\n",
      "Loss function:  tensor(9.2737)\n",
      "Loss function:  tensor(10.3081)\n",
      "Loss function:  tensor(7.0458)\n",
      "Loss function:  tensor(8.9053)\n",
      "Loss function:  tensor(8.2321)\n",
      "Loss function:  tensor(9.8666)\n",
      "Loss function:  tensor(11.7782)\n",
      "Loss function:  tensor(6.8253)\n",
      "Loss function:  tensor(6.9188)\n",
      "Loss function:  tensor(8.1144)\n",
      "Loss function:  tensor(8.2115)\n",
      "Loss function:  tensor(9.2096)\n",
      "Loss function:  tensor(11.7762)\n",
      "Loss function:  tensor(10.5691)\n",
      "Loss function:  tensor(11.6763)\n",
      "Loss function:  tensor(9.2051)\n",
      "Loss function:  tensor(14.1739)\n",
      "Loss function:  tensor(6.1044)\n",
      "Loss function:  tensor(9.7703)\n",
      "Loss function:  tensor(4.9020)\n",
      "Loss function:  tensor(5.7212)\n",
      "Loss function:  tensor(6.4850)\n",
      "Loss function:  tensor(11.6647)\n",
      "Loss function:  tensor(19.5096)\n",
      "Loss function:  tensor(6.0693)\n",
      "Loss function:  tensor(7.2137)\n",
      "Loss function:  tensor(7.2613)\n",
      "Loss function:  tensor(11.9983)\n",
      "Loss function:  tensor(14.1596)\n",
      "Loss function:  tensor(10.0874)\n",
      "Loss function:  tensor(11.1072)\n",
      "Loss function:  tensor(6.6494)\n",
      "Loss function:  tensor(5.0390)\n",
      "Loss function:  tensor(5.9512)\n",
      "Loss function:  tensor(6.3436)\n",
      "Loss function:  tensor(7.8338)\n",
      "Loss function:  tensor(13.4630)\n",
      "Loss function:  tensor(7.8914)\n",
      "Loss function:  tensor(6.0941)\n",
      "Loss function:  tensor(8.5588)\n",
      "Loss function:  tensor(13.4487)\n",
      "Loss function:  tensor(8.5735)\n",
      "Loss function:  tensor(13.4375)\n",
      "Loss function:  tensor(9.6469)\n",
      "Loss function:  tensor(9.5117)\n",
      "Loss function:  tensor(10.2744)\n",
      "Loss function:  tensor(9.3532)\n",
      "Loss function:  tensor(8.0841)\n",
      "Loss function:  tensor(8.9664)\n",
      "Loss function:  tensor(12.9552)\n",
      "Loss function:  tensor(9.7537)\n",
      "Loss function:  tensor(12.3954)\n",
      "Loss function:  tensor(8.7548)\n",
      "Loss function:  tensor(10.6454)\n",
      "Loss function:  tensor(8.8047)\n",
      "Loss function:  tensor(5.5652)\n",
      "Loss function:  tensor(11.6352)\n",
      "Loss function:  tensor(10.3865)\n",
      "Loss function:  tensor(10.0080)\n",
      "Loss function:  tensor(7.1903)\n",
      "Loss function:  tensor(9.2273)\n",
      "Loss function:  tensor(10.5248)\n",
      "Loss function:  tensor(13.7377)\n",
      "Loss function:  tensor(12.2823)\n",
      "Loss function:  tensor(16.0571)\n",
      "Loss function:  tensor(12.9641)\n",
      "Loss function:  tensor(10.4834)\n",
      "Loss function:  tensor(8.1643)\n",
      "Loss function:  tensor(7.2041)\n",
      "Loss function:  tensor(6.7544)\n",
      "Loss function:  tensor(7.6042)\n",
      "Loss function:  tensor(9.8090)\n",
      "Loss function:  tensor(7.4206)\n",
      "Loss function:  tensor(12.7658)\n",
      "Loss function:  tensor(6.4338)\n",
      "Loss function:  tensor(5.5339)\n",
      "Loss function:  tensor(8.9140)\n",
      "Loss function:  tensor(6.7250)\n",
      "Loss function:  tensor(9.3051)\n",
      "Loss function:  tensor(8.5781)\n",
      "Loss function:  tensor(16.0986)\n",
      "Loss function:  tensor(5.4770)\n",
      "Loss function:  tensor(5.6884)\n",
      "Loss function:  tensor(5.1993)\n",
      "Loss function:  tensor(5.8192)\n",
      "Loss function:  tensor(3.0039)\n",
      "Loss function:  tensor(2.1799)\n",
      "Loss function:  tensor(12.8928)\n",
      "Loss function:  tensor(8.9300)\n",
      "Loss function:  tensor(12.8609)\n",
      "Loss function:  tensor(5.5433)\n",
      "Loss function:  tensor(7.8833)\n",
      "Loss function:  tensor(7.2285)\n",
      "Loss function:  tensor(10.3040)\n",
      "Loss function:  tensor(7.6072)\n",
      "Loss function:  tensor(13.0055)\n",
      "Loss function:  tensor(10.4510)\n",
      "Loss function:  tensor(10.7622)\n",
      "Loss function:  tensor(13.8859)\n",
      "Loss function:  tensor(8.2298)\n",
      "Loss function:  tensor(11.9742)\n",
      "Loss function:  tensor(7.8573)\n",
      "Loss function:  tensor(6.7151)\n",
      "Loss function:  tensor(9.8348)\n",
      "Loss function:  tensor(5.1111)\n",
      "Loss function:  tensor(4.7496)\n",
      "Loss function:  tensor(3.8103)\n",
      "Loss function:  tensor(5.2700)\n",
      "Loss function:  tensor(11.4384)\n",
      "Loss function:  tensor(12.6958)\n",
      "Loss function:  tensor(5.7314)\n",
      "Loss function:  tensor(11.2324)\n",
      "Loss function:  tensor(9.9640)\n",
      "Loss function:  tensor(10.8829)\n",
      "Loss function:  tensor(5.4258)\n",
      "Loss function:  tensor(9.2203)\n",
      "Loss function:  tensor(12.6273)\n",
      "Loss function:  tensor(19.9075)\n",
      "Loss function:  tensor(8.5724)\n",
      "Loss function:  tensor(11.4376)\n",
      "Loss function:  tensor(12.4943)\n",
      "Loss function:  tensor(7.7849)\n",
      "Loss function:  tensor(10.7033)\n",
      "Loss function:  tensor(11.0335)\n",
      "Loss function:  tensor(13.2042)\n",
      "Loss function:  tensor(9.3846)\n",
      "Loss function:  tensor(11.9526)\n",
      "Loss function:  tensor(12.4602)\n",
      "Loss function:  tensor(9.2012)\n",
      "Loss function:  tensor(10.1978)\n",
      "Loss function:  tensor(9.6026)\n",
      "Loss function:  tensor(8.6910)\n",
      "Loss function:  tensor(9.8259)\n",
      "Loss function:  tensor(11.8273)\n",
      "Loss function:  tensor(8.3132)\n",
      "Loss function:  tensor(10.5791)\n",
      "Loss function:  tensor(14.5535)\n",
      "Loss function:  tensor(22.0529)\n",
      "Loss function:  tensor(18.5035)\n",
      "Loss function:  tensor(11.8539)\n",
      "Loss function:  tensor(9.2223)\n",
      "Loss function:  tensor(11.8011)\n",
      "Loss function:  tensor(15.2857)\n",
      "Loss function:  tensor(13.5319)\n",
      "Loss function:  tensor(10.8014)\n",
      "Loss function:  tensor(10.6794)\n",
      "Loss function:  tensor(10.3430)\n",
      "Loss function:  tensor(11.7875)\n",
      "Loss function:  tensor(7.4048)\n",
      "Loss function:  tensor(14.4997)\n",
      "Loss function:  tensor(7.3552)\n",
      "Loss function:  tensor(7.0686)\n",
      "Loss function:  tensor(5.9403)\n",
      "Loss function:  tensor(11.5979)\n",
      "Loss function:  tensor(7.4581)\n",
      "Loss function:  tensor(7.0024)\n",
      "Loss function:  tensor(4.7289)\n",
      "Loss function:  tensor(6.2867)\n",
      "Loss function:  tensor(9.5538)\n",
      "Loss function:  tensor(6.2742)\n",
      "Loss function:  tensor(6.7592)\n",
      "Loss function:  tensor(8.3024)\n",
      "Loss function:  tensor(8.4241)\n",
      "Loss function:  tensor(11.7811)\n",
      "Loss function:  tensor(6.1489)\n",
      "Loss function:  tensor(13.8128)\n",
      "Loss function:  tensor(6.0840)\n",
      "Loss function:  tensor(8.8668)\n",
      "Loss function:  tensor(5.9740)\n",
      "Loss function:  tensor(9.8203)\n",
      "Loss function:  tensor(8.6277)\n",
      "Loss function:  tensor(8.4430)\n",
      "Loss function:  tensor(10.9278)\n",
      "Loss function:  tensor(11.8637)\n",
      "Loss function:  tensor(11.3840)\n",
      "Loss function:  tensor(9.2615)\n",
      "Loss function:  tensor(8.8022)\n",
      "Loss function:  tensor(11.9270)\n",
      "Loss function:  tensor(9.7111)\n",
      "Loss function:  tensor(11.6503)\n",
      "Loss function:  tensor(8.1417)\n",
      "Loss function:  tensor(10.5792)\n",
      "Loss function:  tensor(9.7074)\n",
      "Loss function:  tensor(7.6121)\n",
      "Loss function:  tensor(7.6892)\n",
      "Loss function:  tensor(10.0089)\n",
      "Loss function:  tensor(10.3256)\n",
      "Loss function:  tensor(9.5621)\n",
      "Loss function:  tensor(11.0466)\n",
      "Loss function:  tensor(11.8127)\n",
      "Loss function:  tensor(7.8099)\n",
      "Loss function:  tensor(13.8344)\n",
      "Loss function:  tensor(12.4217)\n",
      "Loss function:  tensor(13.0763)\n",
      "Loss function:  tensor(7.8404)\n",
      "Loss function:  tensor(10.9979)\n",
      "Loss function:  tensor(10.9120)\n",
      "Loss function:  tensor(5.3545)\n",
      "Loss function:  tensor(4.9536)\n",
      "Loss function:  tensor(6.8739)\n",
      "Loss function:  tensor(9.7800)\n",
      "Loss function:  tensor(10.3273)\n",
      "Loss function:  tensor(6.0964)\n",
      "Loss function:  tensor(5.2084)\n",
      "Loss function:  tensor(6.9509)\n",
      "Loss function:  tensor(10.0440)\n",
      "Loss function:  tensor(13.2778)\n",
      "Loss function:  tensor(9.2567)\n",
      "Loss function:  tensor(10.7840)\n",
      "Loss function:  tensor(12.6373)\n",
      "Loss function:  tensor(9.7444)\n",
      "Loss function:  tensor(14.8712)\n",
      "Loss function:  tensor(11.0053)\n",
      "Loss function:  tensor(7.9072)\n",
      "Loss function:  tensor(8.1876)\n",
      "Loss function:  tensor(15.6683)\n",
      "Loss function:  tensor(18.0793)\n",
      "Loss function:  tensor(15.3135)\n",
      "Loss function:  tensor(12.5648)\n",
      "Loss function:  tensor(5.8768)\n",
      "Loss function:  tensor(7.1371)\n",
      "Loss function:  tensor(8.8245)\n",
      "Loss function:  tensor(8.2244)\n",
      "Loss function:  tensor(12.8690)\n",
      "Loss function:  tensor(13.6354)\n",
      "Loss function:  tensor(7.9606)\n",
      "Loss function:  tensor(7.4978)\n",
      "Loss function:  tensor(9.2201)\n",
      "Loss function:  tensor(9.4110)\n",
      "Loss function:  tensor(11.1669)\n",
      "Loss function:  tensor(12.3326)\n",
      "Loss function:  tensor(8.6991)\n",
      "Loss function:  tensor(10.1931)\n",
      "Loss function:  tensor(3.0004)\n",
      "Loss function:  tensor(3.7991)\n",
      "Loss function:  tensor(10.1084)\n",
      "Loss function:  tensor(9.8376)\n",
      "Loss function:  tensor(8.5557)\n",
      "Loss function:  tensor(10.0665)\n",
      "Loss function:  tensor(12.7257)\n",
      "Loss function:  tensor(11.3884)\n",
      "Loss function:  tensor(6.6398)\n",
      "Loss function:  tensor(11.9078)\n",
      "Loss function:  tensor(8.6718)\n",
      "Loss function:  tensor(8.0027)\n",
      "Loss function:  tensor(11.4797)\n",
      "Loss function:  tensor(6.9053)\n",
      "Loss function:  tensor(6.7268)\n",
      "Loss function:  tensor(8.9787)\n",
      "Loss function:  tensor(5.3318)\n",
      "Loss function:  tensor(3.9920)\n",
      "Loss function:  tensor(11.9148)\n",
      "Loss function:  tensor(9.3117)\n",
      "Loss function:  tensor(10.9292)\n",
      "Loss function:  tensor(14.4825)\n",
      "Loss function:  tensor(6.4929)\n",
      "Loss function:  tensor(6.4813)\n",
      "Loss function:  tensor(12.0537)\n",
      "Loss function:  tensor(8.8653)\n",
      "Loss function:  tensor(4.6571)\n",
      "Loss function:  tensor(13.3883)\n",
      "Loss function:  tensor(9.1067)\n",
      "Loss function:  tensor(8.9231)\n",
      "Loss function:  tensor(11.2022)\n",
      "Loss function:  tensor(12.4535)\n",
      "Loss function:  tensor(14.4035)\n",
      "Loss function:  tensor(10.6751)\n",
      "Loss function:  tensor(13.2215)\n",
      "Loss function:  tensor(10.4767)\n",
      "Loss function:  tensor(9.6577)\n",
      "Loss function:  tensor(15.9277)\n",
      "Loss function:  tensor(6.5505)\n",
      "Loss function:  tensor(8.6764)\n",
      "Loss function:  tensor(9.2141)\n",
      "Loss function:  tensor(5.2922)\n",
      "Loss function:  tensor(11.2070)\n",
      "Loss function:  tensor(10.7117)\n",
      "Loss function:  tensor(12.2188)\n",
      "Loss function:  tensor(10.3633)\n",
      "Loss function:  tensor(6.0667)\n",
      "Loss function:  tensor(13.0675)\n",
      "Loss function:  tensor(8.6069)\n",
      "Loss function:  tensor(7.0217)\n",
      "Loss function:  tensor(8.6878)\n",
      "Loss function:  tensor(5.8782)\n",
      "Loss function:  tensor(11.3097)\n",
      "Loss function:  tensor(6.1238)\n",
      "Loss function:  tensor(20.7221)\n",
      "Loss function:  tensor(7.5433)\n",
      "Loss function:  tensor(6.3253)\n",
      "Loss function:  tensor(6.8734)\n",
      "Loss function:  tensor(3.7733)\n",
      "Loss function:  tensor(10.3754)\n",
      "Loss function:  tensor(10.9536)\n",
      "Loss function:  tensor(11.6586)\n",
      "Loss function:  tensor(7.0106)\n",
      "Loss function:  tensor(8.0390)\n",
      "Loss function:  tensor(7.3682)\n",
      "Loss function:  tensor(9.7220)\n",
      "Loss function:  tensor(7.0392)\n",
      "Loss function:  tensor(14.0142)\n",
      "Loss function:  tensor(9.0508)\n",
      "Loss function:  tensor(9.1713)\n",
      "Loss function:  tensor(11.9804)\n",
      "Loss function:  tensor(9.3704)\n",
      "Loss function:  tensor(9.6086)\n",
      "Loss function:  tensor(7.4015)\n",
      "Loss function:  tensor(8.1160)\n",
      "Loss function:  tensor(12.3091)\n",
      "Loss function:  tensor(6.6549)\n",
      "Loss function:  tensor(12.0038)\n",
      "Loss function:  tensor(6.7527)\n",
      "Loss function:  tensor(10.9846)\n",
      "Loss function:  tensor(12.8980)\n",
      "Loss function:  tensor(7.9305)\n",
      "Loss function:  tensor(4.6812)\n",
      "Loss function:  tensor(6.0773)\n",
      "Loss function:  tensor(4.2642)\n",
      "Loss function:  tensor(9.0451)\n",
      "Loss function:  tensor(5.7603)\n",
      "Loss function:  tensor(11.7102)\n",
      "Loss function:  tensor(12.4419)\n",
      "Loss function:  tensor(7.6016)\n",
      "Loss function:  tensor(9.8501)\n",
      "Loss function:  tensor(8.2806)\n",
      "Loss function:  tensor(6.7557)\n",
      "Loss function:  tensor(6.5457)\n",
      "Loss function:  tensor(6.8006)\n",
      "Loss function:  tensor(9.8627)\n",
      "Loss function:  tensor(8.9119)\n",
      "Loss function:  tensor(6.7592)\n",
      "Loss function:  tensor(5.8239)\n",
      "Loss function:  tensor(7.8020)\n",
      "Loss function:  tensor(7.0634)\n",
      "Loss function:  tensor(9.2408)\n",
      "Loss function:  tensor(10.3322)\n",
      "Loss function:  tensor(14.5353)\n",
      "Loss function:  tensor(4.7945)\n",
      "Loss function:  tensor(8.1730)\n",
      "Loss function:  tensor(7.4690)\n",
      "Loss function:  tensor(5.4409)\n",
      "Loss function:  tensor(7.1316)\n",
      "Loss function:  tensor(5.4863)\n",
      "Loss function:  tensor(6.0673)\n",
      "Loss function:  tensor(14.2272)\n",
      "Loss function:  tensor(11.8761)\n",
      "Loss function:  tensor(10.8138)\n",
      "Loss function:  tensor(7.2524)\n",
      "Loss function:  tensor(8.4681)\n",
      "Loss function:  tensor(10.3370)\n",
      "Loss function:  tensor(11.5598)\n",
      "Loss function:  tensor(9.3948)\n",
      "Loss function:  tensor(8.0563)\n",
      "Loss function:  tensor(10.2137)\n",
      "Loss function:  tensor(7.7790)\n",
      "Loss function:  tensor(5.1196)\n",
      "Loss function:  tensor(7.3952)\n",
      "Loss function:  tensor(6.7228)\n",
      "Loss function:  tensor(7.8499)\n",
      "Loss function:  tensor(4.4597)\n",
      "Loss function:  tensor(3.4281)\n",
      "Loss function:  tensor(5.8464)\n",
      "Loss function:  tensor(8.5249)\n",
      "Loss function:  tensor(9.6426)\n",
      "Loss function:  tensor(5.8633)\n",
      "Loss function:  tensor(6.0525)\n"
     ]
    }
   ],
   "source": [
    "ClipThreshold = 5.\n",
    "\n",
    "model = TaggerCRFNet(dataset.get_vocab_size(), 10, dataset.get_tagset_size())\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=collate_fn)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(1):  # dla testów wystarczy jedna epoka, możesz przerwać obliczenia w jej trakcie\n",
    "  loss_sum = torch.tensor([0.])\n",
    "  for i, (sentences, tags, lengths) in enumerate(dataloader):\n",
    "    loss = -model.seq_log_probability(sentences.T, tags.T, lengths)\n",
    "    loss.backward()\n",
    "\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), ClipThreshold)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      loss_sum += loss[0]\n",
    "      if i % 10 == 9:\n",
    "        print(\"Loss function: \", loss_sum / 10)\n",
    "        # Wyniki nadal będą niestabilne, ale ogólna tendencja liczb powinna być malejąca\n",
    "        loss_sum = 0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
